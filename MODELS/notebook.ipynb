{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T12:22:20.805411Z",
     "start_time": "2025-08-19T12:22:05.332269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para o arquivo CSV\n",
    "caminho = r'C:\\Users\\Murilo\\Documents\\dados_desafio_fiap\\hash\\df_t.csv'\n",
    "\n",
    "# Leitura do arquivo CSV\n",
    "df = pd.read_csv(caminho)\n",
    "\n",
    "# Exibe a tabela original\n",
    "print(\"Tabela original:\")\n",
    "print(df.head())\n",
    "\n",
    "# =========================\n",
    "# CURA E TRANSFORMAÇÃO\n",
    "# =========================\n",
    "\n",
    "df_curado = df.copy()\n",
    "\n",
    "# 1. Garantir que os IDs são strings\n",
    "df_curado['nk_ota_localizer_id'] = df_curado['nk_ota_localizer_id'].astype(str)\n",
    "df_curado['fk_contact'] = df_curado['fk_contact'].astype(str)\n",
    "\n",
    "# 2. Criar o campo datetime unificando data e hora\n",
    "df_curado['data_hora_compra'] = pd.to_datetime(\n",
    "    df_curado['date_purchase'] + ' ' + df_curado['time_purchase'], errors='coerce'\n",
    ")\n",
    "\n",
    "# 3. Trocar \"0\" por np.nan nos campos de retorno\n",
    "campos_com_zero_para_nulo = ['place_origin_return', 'place_destination_return']\n",
    "for col in campos_com_zero_para_nulo:\n",
    "    df_curado[col] = df_curado[col].replace(\"0\", np.nan)\n",
    "\n",
    "# Trocar \"0\" e \"1\" por np.nan na fk_return_ota_bus_company\n",
    "df_curado['fk_return_ota_bus_company'] = df_curado['fk_return_ota_bus_company'].replace([\"0\", \"1\"], np.nan)\n",
    "\n",
    "# 4. Criar coluna \"classificacao_viagem\"\n",
    "df_curado['classificacao_viagem'] = df_curado['place_origin_return'].apply(\n",
    "    lambda x: 'ida_e_volta' if pd.notna(x) else 'ida'\n",
    ")\n",
    "\n",
    "# 5. Garantir que os tipos de gmv e tickets estão corretos\n",
    "df_curado['gmv_success'] = df_curado['gmv_success'].astype(float)\n",
    "df_curado['total_tickets_quantity_success'] = df_curado['total_tickets_quantity_success'].astype('Int64')\n",
    "\n",
    "# 6. Remover colunas antigas de data e hora\n",
    "df_curado.drop(['date_purchase', 'time_purchase'], axis=1, inplace=True)\n",
    "\n",
    "# 7. Criar colunas adicionais\n",
    "df_curado['tipo_compra'] = df_curado['total_tickets_quantity_success'].apply(\n",
    "    lambda x: 'individual' if x == 1 else 'coletiva'\n",
    ")\n",
    "df_curado['sem_retorno_flag'] = df_curado['place_origin_return'].isnull()\n",
    "df_curado['compra_dia_util'] = df_curado['data_hora_compra'].dt.weekday < 5\n",
    "\n",
    "# Classificação por período do dia\n",
    "def classificar_periodo(hora):\n",
    "    if 0 <= hora < 6:\n",
    "        return 'madrugada'\n",
    "    elif 6 <= hora < 12:\n",
    "        return 'manhã'\n",
    "    elif 12 <= hora < 18:\n",
    "        return 'tarde'\n",
    "    else:\n",
    "        return 'noite'\n",
    "\n",
    "df_curado['hora_periodo'] = df_curado['data_hora_compra'].dt.hour.apply(classificar_periodo)\n",
    "\n",
    "# 8. Verifica a data da primeira compra por cliente\n",
    "primeiras_compras = df_curado.groupby('fk_contact')['data_hora_compra'].min().reset_index()\n",
    "primeiras_compras['primeira_compra'] = True\n",
    "\n",
    "df_curado = df_curado.merge(primeiras_compras, on=['fk_contact', 'data_hora_compra'], how='left')\n",
    "df_curado['primeira_compra'] = df_curado['primeira_compra'].fillna(False)\n",
    "\n",
    "# 9. Renomear colunas\n",
    "df_curado.rename(columns={\n",
    "    'nk_ota_localizer_id': 'order_id',\n",
    "    'fk_contact': 'client_id',\n",
    "    'place_origin_departure': 'origin_departure',\n",
    "    'place_destination_departure': 'destination_departure',\n",
    "    'place_origin_return': 'origin_return',\n",
    "    'place_destination_return': 'destination_return',\n",
    "    'fk_departure_ota_bus_company': 'bus_company_departure',\n",
    "    'fk_return_ota_bus_company': 'bus_company_return',\n",
    "    'gmv_success': 'total_value',\n",
    "    'total_tickets_quantity_success': 'tickets_quantity',\n",
    "    'data_hora_compra': 'purchase_datetime',\n",
    "    'classificacao_viagem': 'trip_type',\n",
    "    'tipo_compra': 'purchase_type',\n",
    "    'sem_retorno_flag': 'no_return_flag',\n",
    "    'compra_dia_util': 'purchase_weekday_flag',\n",
    "    'hora_periodo': 'purchase_time_period',\n",
    "    'primeira_compra': 'first_purchase_flag'\n",
    "}, inplace=True)\n",
    "\n",
    "# 10. Reordenar colunas\n",
    "nova_ordem = [\n",
    "    'purchase_datetime', 'order_id', 'client_id', 'purchase_weekday_flag', 'purchase_time_period',\n",
    "    'first_purchase_flag', 'purchase_type', 'tickets_quantity', 'total_value', 'trip_type', 'no_return_flag',\n",
    "    'origin_departure', 'destination_departure', 'origin_return', 'destination_return',\n",
    "    'bus_company_departure', 'bus_company_return'\n",
    "]\n",
    "\n",
    "df_curado = df_curado[nova_ordem]\n",
    "\n",
    "# =========================\n",
    "# RESULTADOS\n",
    "# =========================\n",
    "\n",
    "print(\"\\nTabela curada:\")\n",
    "print(df_curado.head())\n",
    "\n",
    "# Horas únicas por período\n",
    "horas_por_periodo = df_curado.groupby('purchase_time_period')['purchase_datetime'].apply(\n",
    "    lambda x: sorted(x.dt.hour.unique())\n",
    ")\n",
    "\n",
    "print(\"\\nHoras por período:\")\n",
    "for periodo, horas in horas_por_periodo.items():\n",
    "    print(f\"{periodo}: {horas}\")\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela original:\n",
      "                                 nk_ota_localizer_id  \\\n",
      "0  bc02d5245bec63b30ff1102fa273fc03f58bc9cc3f674e...   \n",
      "1  5432f12612dd5d749b3be880e779989cf63b5efa4bcc4e...   \n",
      "2  fb3caed9b2f1b6016d45ccddb19095476e61a2c85faa8e...   \n",
      "3  4dc44a6dd592b702feccb493d192210c86965aee684529...   \n",
      "4  aa34ed7fd0a6b405df2df1bf9f8d68e6df9b9a868a6181...   \n",
      "\n",
      "                                          fk_contact date_purchase  \\\n",
      "0  a7218ff4ee7d37d48d2b4391b955627cb089870b934912...    2018-12-26   \n",
      "1  37228485e0dc83d84d1bcd1bef3dc632301bf6cb22c8b5...    2018-12-05   \n",
      "2  3467ec081e2421e72c96e7203b929d21927fd00b6b5f28...    2018-12-21   \n",
      "3  ab3251a2be0f69713b8f97b0e9d1579e31551f4fd4facf...    2018-12-06   \n",
      "4  ceea0de820a6379f2c4215bddaec66c33994b304607e56...    2021-02-23   \n",
      "\n",
      "  time_purchase                             place_origin_departure  \\\n",
      "0      15:33:35  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...   \n",
      "1      15:07:57  10e4e7caf8b078429bb1c80b1a10118ac6f963eff098fd...   \n",
      "2      18:41:54  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "3      14:01:38  4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...   \n",
      "4      20:08:25  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "\n",
      "                         place_destination_departure place_origin_return  \\\n",
      "0  50e9a8665b62c8d68bccc77c7c92431a1aa26ccbd38ed4...                   0   \n",
      "1  e6d41d208672a4e50b86d959f4a6254975e6fb9b088116...                   0   \n",
      "2  8c1f1046219ddd216a023f792356ddf127fce372a72ec9...                   0   \n",
      "3  d6acb3c1a79e57bcc03d976cb4d98f56edccd4cf426392...                   0   \n",
      "4  23765fc69c4e3c0b10f5d15471dc2245e2a19af16b513f...                   0   \n",
      "\n",
      "  place_destination_return                       fk_departure_ota_bus_company  \\\n",
      "0                        0  8527a891e224136950ff32ca212b45bc93f69fbb801c3b...   \n",
      "1                        0  36ebe205bcdfc499a25e6923f4450fa8d48196ceb4fa0c...   \n",
      "2                        0  ec2e990b934dde55cb87300629cedfc21b15cd28bbcf77...   \n",
      "3                        0  5f9c4ab08cac7457e9111a30e4664920607ea2c115a143...   \n",
      "4                        0  48449a14a4ff7d79bb7a1b6f3d488eba397c36ef25634c...   \n",
      "\n",
      "  fk_return_ota_bus_company  gmv_success  total_tickets_quantity_success  \n",
      "0                         1        89.09                               1  \n",
      "1                         1       155.97                               1  \n",
      "2                         1       121.99                               1  \n",
      "3                         1        55.22                               1  \n",
      "4                         1        45.31                               1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\AppData\\Local\\Temp\\ipykernel_34724\\4149907296.py:74: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_curado['primeira_compra'] = df_curado['primeira_compra'].fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela curada:\n",
      "    purchase_datetime                                           order_id  \\\n",
      "0 2018-12-26 15:33:35  bc02d5245bec63b30ff1102fa273fc03f58bc9cc3f674e...   \n",
      "1 2018-12-05 15:07:57  5432f12612dd5d749b3be880e779989cf63b5efa4bcc4e...   \n",
      "2 2018-12-21 18:41:54  fb3caed9b2f1b6016d45ccddb19095476e61a2c85faa8e...   \n",
      "3 2018-12-06 14:01:38  4dc44a6dd592b702feccb493d192210c86965aee684529...   \n",
      "4 2021-02-23 20:08:25  aa34ed7fd0a6b405df2df1bf9f8d68e6df9b9a868a6181...   \n",
      "\n",
      "                                           client_id  purchase_weekday_flag  \\\n",
      "0  a7218ff4ee7d37d48d2b4391b955627cb089870b934912...                   True   \n",
      "1  37228485e0dc83d84d1bcd1bef3dc632301bf6cb22c8b5...                   True   \n",
      "2  3467ec081e2421e72c96e7203b929d21927fd00b6b5f28...                   True   \n",
      "3  ab3251a2be0f69713b8f97b0e9d1579e31551f4fd4facf...                   True   \n",
      "4  ceea0de820a6379f2c4215bddaec66c33994b304607e56...                   True   \n",
      "\n",
      "  purchase_time_period  first_purchase_flag purchase_type  tickets_quantity  \\\n",
      "0                tarde                 True    individual                 1   \n",
      "1                tarde                False    individual                 1   \n",
      "2                noite                False    individual                 1   \n",
      "3                tarde                False    individual                 1   \n",
      "4                noite                 True    individual                 1   \n",
      "\n",
      "   total_value trip_type  no_return_flag  \\\n",
      "0        89.09       ida            True   \n",
      "1       155.97       ida            True   \n",
      "2       121.99       ida            True   \n",
      "3        55.22       ida            True   \n",
      "4        45.31       ida            True   \n",
      "\n",
      "                                    origin_departure  \\\n",
      "0  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...   \n",
      "1  10e4e7caf8b078429bb1c80b1a10118ac6f963eff098fd...   \n",
      "2  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "3  4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...   \n",
      "4  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "\n",
      "                               destination_departure origin_return  \\\n",
      "0  50e9a8665b62c8d68bccc77c7c92431a1aa26ccbd38ed4...           NaN   \n",
      "1  e6d41d208672a4e50b86d959f4a6254975e6fb9b088116...           NaN   \n",
      "2  8c1f1046219ddd216a023f792356ddf127fce372a72ec9...           NaN   \n",
      "3  d6acb3c1a79e57bcc03d976cb4d98f56edccd4cf426392...           NaN   \n",
      "4  23765fc69c4e3c0b10f5d15471dc2245e2a19af16b513f...           NaN   \n",
      "\n",
      "  destination_return                              bus_company_departure  \\\n",
      "0                NaN  8527a891e224136950ff32ca212b45bc93f69fbb801c3b...   \n",
      "1                NaN  36ebe205bcdfc499a25e6923f4450fa8d48196ceb4fa0c...   \n",
      "2                NaN  ec2e990b934dde55cb87300629cedfc21b15cd28bbcf77...   \n",
      "3                NaN  5f9c4ab08cac7457e9111a30e4664920607ea2c115a143...   \n",
      "4                NaN  48449a14a4ff7d79bb7a1b6f3d488eba397c36ef25634c...   \n",
      "\n",
      "  bus_company_return  \n",
      "0                NaN  \n",
      "1                NaN  \n",
      "2                NaN  \n",
      "3                NaN  \n",
      "4                NaN  \n",
      "\n",
      "Horas por período:\n",
      "madrugada: [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5)]\n",
      "manhã: [np.int32(6), np.int32(7), np.int32(8), np.int32(9), np.int32(10), np.int32(11)]\n",
      "noite: [np.int32(18), np.int32(19), np.int32(20), np.int32(21), np.int32(22), np.int32(23)]\n",
      "tarde: [np.int32(12), np.int32(13), np.int32(14), np.int32(15), np.int32(16), np.int32(17)]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:10:40.098221Z",
     "start_time": "2025-08-19T15:06:17.380082Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # ================================================\n",
    "# TREINO RÁPIDO + SALVAR MODELO + CSVs (holdout/full)\n",
    "# ================================================\n",
    "import os, json, math, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "try:\n",
    "    from lightgbm import early_stopping, log_evaluation\n",
    "    _HAS_EARLY = True\n",
    "except Exception:\n",
    "    _HAS_EARLY = False\n",
    "    early_stopping = None\n",
    "    log_evaluation = None\n",
    "\n",
    "# -----------------\n",
    "# CONFIG (rápido)\n",
    "# -----------------\n",
    "RANDOM_STATE   = 42\n",
    "TEST_SIZE      = 0.20       # holdout por cliente\n",
    "VAL_SIZE       = 0.10       # validação interna\n",
    "TOP_K          = 3\n",
    "\n",
    "# redução de classes (quanto mais agressivo, mais rápido)\n",
    "COVERAGE       = 0.90       # cobre ~90% do volume\n",
    "MIN_COUNT      = 300        # classes com < MIN_COUNT -> \"__OUTROS__\"\n",
    "\n",
    "# subamostra do conjunto de treino (acelera muito o early stopping)\n",
    "TRAIN_FRAC     = 0.30       # 30% do treino\n",
    "\n",
    "# lightgbm (rápido)\n",
    "EARLY_ROUNDS   = 50         # para cedo se não melhorar\n",
    "N_ESTIMATORS   = 3000\n",
    "LEARNING_RATE  = 0.12\n",
    "NUM_LEAVES     = 63\n",
    "MAX_BIN        = 63\n",
    "MIN_DATA_LEAF  = 200\n",
    "FEATURE_FRAC   = 0.8\n",
    "\n",
    "# saída\n",
    "ARTS_DIR       = \"artifacts_destino_model\"\n",
    "CSV_HOLDOUT    = \"predicoes_holdout.csv\"\n",
    "CSV_FULL       = \"predicoes_full.csv\"\n",
    "os.makedirs(ARTS_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------\n",
    "# Helpers\n",
    "# -----------------\n",
    "def pick_base_df(df_curado, df_sample=None):\n",
    "    if ('df_sample' in globals()\n",
    "        and isinstance(df_sample, pd.DataFrame)\n",
    "        and not df_sample.empty):\n",
    "        print(f\"[INFO] Usando df_sample: {len(df_sample):,} linhas\")\n",
    "        return df_sample.copy()\n",
    "    if df_curado is None or not isinstance(df_curado, pd.DataFrame) or df_curado.empty:\n",
    "        raise ValueError(\"df_curado não encontrado ou vazio. Rode o bloco de limpeza antes.\")\n",
    "    print(f\"[INFO] Usando df_curado: {len(df_curado):,} linhas\")\n",
    "    return df_curado.copy()\n",
    "\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df['purchase_datetime'] = pd.to_datetime(df['purchase_datetime'], errors='coerce')\n",
    "    df = df.sort_values(['client_id','purchase_datetime']).reset_index(drop=True)\n",
    "\n",
    "    # days_since_last\n",
    "    df['days_since_last'] = df.groupby('client_id')['purchase_datetime'].diff().dt.days\n",
    "    df['days_since_last'] = df['days_since_last'].fillna(\n",
    "        df.groupby('client_id')['days_since_last'].transform('median')\n",
    "    )\n",
    "    df['days_since_last'] = df['days_since_last'].fillna(df['days_since_last'].median())\n",
    "\n",
    "    # freq e ticket médio\n",
    "    freq = (df.groupby('client_id', as_index=False)['order_id']\n",
    "              .count().rename(columns={'order_id':'client_total_orders'}))\n",
    "    avgv = (df.groupby('client_id', as_index=False)['total_value']\n",
    "              .mean().rename(columns={'total_value':'client_avg_value'}))\n",
    "    df = df.merge(freq, on='client_id', how='left').merge(avgv, on='client_id', how='left')\n",
    "\n",
    "    # flags / tempo\n",
    "    if df['purchase_weekday_flag'].dtype == bool:\n",
    "        df['purchase_weekday_flag'] = df['purchase_weekday_flag'].astype(int)\n",
    "    df['purchase_month'] = df['purchase_datetime'].dt.month.astype('Int64')\n",
    "    df['purchase_hour']  = df['purchase_datetime'].dt.hour.astype('Int64')\n",
    "    return df\n",
    "\n",
    "def reduce_classes(df, target):\n",
    "    counts  = df[target].astype(str).value_counts()\n",
    "    cumcov  = counts.cumsum() / counts.sum()\n",
    "    keep    = set(counts.index[(cumcov <= COVERAGE) | (counts >= MIN_COUNT)])\n",
    "    df[target] = df[target].astype(str).where(df[target].astype(str).isin(keep), \"__OUTROS__\")\n",
    "    cov_reached = float(counts[counts.index.isin(keep)].sum() / counts.sum())\n",
    "    print(f\"[INFO] Destinos mantidos: {len(keep)} | cobertura≈{cov_reached:.3f} | '__OUTROS__' aplicado ao restante\")\n",
    "    return df\n",
    "\n",
    "def make_X(df, features, cat_cols):\n",
    "    X = df[features].copy()\n",
    "    for c in cat_cols:\n",
    "        X[c] = X[c].astype('category')\n",
    "    return X\n",
    "\n",
    "def top_k_acc(y_true, proba, model_classes, k=3):\n",
    "    \"\"\"Top-k robusto sem depender de sklearn >=1.0\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    k = min(k, proba.shape[1])\n",
    "    idx_map = {c:i for i, c in enumerate(model_classes)}\n",
    "    ac = 0\n",
    "    for yt, p in zip(y_true, proba):\n",
    "        if yt in idx_map:\n",
    "            if idx_map[yt] in np.argpartition(p, -k)[-k:]:\n",
    "                ac += 1\n",
    "    return ac / len(y_true) if len(y_true) else np.nan\n",
    "\n",
    "def score_dataframe_in_batches(model, X_like, df_base_for_ids, id_cols,\n",
    "                               classes_model, le_dest, topk=3, batch_size=250_000):\n",
    "    import json\n",
    "    out_frames = []\n",
    "    n = len(X_like)\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        Xi = X_like.iloc[start:end]\n",
    "        ids = df_base_for_ids.loc[Xi.index, list(id_cols)].copy()\n",
    "\n",
    "        proba = model.predict_proba(Xi)\n",
    "        top1_idx   = proba.argmax(axis=1)\n",
    "        top1_codes = classes_model[top1_idx]\n",
    "        ids[\"pred_top1\"]      = le_dest.inverse_transform(top1_codes)\n",
    "        ids[\"pred_top1_prob\"] = np.round(proba[np.arange(len(proba)), top1_idx] * 100, 2)\n",
    "\n",
    "        k = min(topk, proba.shape[1])\n",
    "        topk_idx   = np.argsort(proba, axis=1)[:, -k:][:, ::-1]\n",
    "        topk_codes = classes_model[topk_idx]\n",
    "        topk_lbls  = [le_dest.inverse_transform(row).tolist() for row in topk_codes]\n",
    "        topk_probs = [list(np.round(proba[i, topk_idx[i]]*100, 2)) for i in range(len(proba))]\n",
    "        ids[\"pred_topk_labels\"] = [json.dumps(v) for v in topk_lbls]\n",
    "        ids[\"pred_topk_probs\"]  = [json.dumps(v) for v in topk_probs]\n",
    "\n",
    "        out_frames.append(ids.reset_index(drop=True))\n",
    "        print(f\"[SCORE] {end:,}/{n:,} linhas processadas...\")\n",
    "    return pd.concat(out_frames, ignore_index=True)\n",
    "\n",
    "# -----------------\n",
    "# 1) Base + features\n",
    "# -----------------\n",
    "t0 = time.time()\n",
    "df_base = pick_base_df(df_curado=df_curado, df_sample=globals().get('df_sample', None))\n",
    "df = engineer_features(df_base)\n",
    "\n",
    "# -----------------\n",
    "# 2) Redução de classes + encoders\n",
    "# -----------------\n",
    "target = 'destination_departure'\n",
    "df = reduce_classes(df, target)\n",
    "\n",
    "le_dest   = LabelEncoder(); df[target] = le_dest.fit_transform(df[target].astype(str))\n",
    "le_period = LabelEncoder(); df['purchase_time_period'] = le_period.fit_transform(df['purchase_time_period'].astype(str))\n",
    "le_origin = LabelEncoder(); df['origin_enc']            = le_origin.fit_transform(df['origin_departure'].astype(str))\n",
    "le_bus    = LabelEncoder(); df['bus_enc']               = le_bus.fit_transform(df['bus_company_departure'].astype(str))\n",
    "\n",
    "features = [\n",
    "    'tickets_quantity','total_value',\n",
    "    'purchase_weekday_flag','purchase_time_period',\n",
    "    'days_since_last','client_total_orders','client_avg_value',\n",
    "    'purchase_month','purchase_hour','origin_enc','bus_enc'\n",
    "]\n",
    "cat_cols = ['purchase_time_period','purchase_month','purchase_hour','origin_enc','bus_enc']\n",
    "\n",
    "X_all = make_X(df, features, cat_cols)\n",
    "y_all = df[target].astype(int).copy()\n",
    "groups_all = df['client_id'].astype(str)\n",
    "\n",
    "if y_all.nunique() < 2:\n",
    "    raise ValueError(\"Target com < 2 classes após redução. Ajuste COVERAGE/MIN_COUNT.\")\n",
    "\n",
    "# -----------------\n",
    "# 3) Split (holdout por cliente) + validação interna\n",
    "# -----------------\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "tr_idx, te_idx = next(gss.split(X_all, y_all, groups=groups_all))\n",
    "X_train_all, X_test = X_all.iloc[tr_idx], X_all.iloc[te_idx]\n",
    "y_train_all, y_test = y_all.iloc[tr_idx], y_all.iloc[te_idx]\n",
    "groups_train = groups_all.iloc[tr_idx]\n",
    "\n",
    "gss_val = GroupShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=RANDOM_STATE)\n",
    "tr_i, val_i = next(gss_val.split(X_train_all, y_train_all, groups=groups_train))\n",
    "X_tr, X_val = X_train_all.iloc[tr_i], X_train_all.iloc[val_i]\n",
    "y_tr, y_val = y_train_all.iloc[tr_i], y_train_all.iloc[val_i]\n",
    "\n",
    "# remove classes não vistas no treino na validação (evita logloss congelado)\n",
    "mask_val = y_val.isin(set(y_tr.unique()))\n",
    "if not mask_val.all():\n",
    "    removed = int((~mask_val).sum())\n",
    "    X_val, y_val = X_val[mask_val], y_val[mask_val]\n",
    "    print(f\"[INFO] Removidas {removed} amostras de validação com classes não vistas no treino.\")\n",
    "\n",
    "# subamostra do treino (acelera MUITO)\n",
    "if TRAIN_FRAC < 1.0:\n",
    "    rs = np.random.RandomState(RANDOM_STATE)\n",
    "    sel = rs.choice(X_tr.index, size=int(len(X_tr)*TRAIN_FRAC), replace=False)\n",
    "    X_tr, y_tr = X_tr.loc[sel], y_tr.loc[sel]\n",
    "    print(f\"[INFO] Subamostrando treino: {len(X_tr):,} linhas\")\n",
    "\n",
    "# -----------------\n",
    "# 4) Modelo e treino rápido (GOSS)\n",
    "# -----------------\n",
    "model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    boosting_type='goss',\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_leaves=NUM_LEAVES,\n",
    "    max_depth=-1,\n",
    "    max_bin=MAX_BIN,\n",
    "    min_data_in_leaf=MIN_DATA_LEAF,\n",
    "    feature_fraction=FEATURE_FRAC,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "fit_kwargs = dict(\n",
    "    X=X_tr, y=y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='multi_logloss',\n",
    "    categorical_feature=cat_cols\n",
    ")\n",
    "if _HAS_EARLY:\n",
    "    fit_kwargs[\"callbacks\"] = [early_stopping(EARLY_ROUNDS), log_evaluation(25)]\n",
    "model.fit(**fit_kwargs)\n",
    "\n",
    "# -----------------\n",
    "# 5) Métrica rápida no holdout\n",
    "# -----------------\n",
    "classes_model = model.classes_\n",
    "mask_test = y_test.isin(set(classes_model))\n",
    "if not mask_test.all():\n",
    "    print(f\"[INFO] Holdout: ignorando {int((~mask_test).sum())} linhas com classes fora do treino para métricas.\")\n",
    "X_test_m, y_test_m = X_test[mask_test], y_test[mask_test]\n",
    "\n",
    "proba_test = model.predict_proba(X_test_m)\n",
    "y_pred_codes = classes_model[proba_test.argmax(axis=1)]\n",
    "\n",
    "acc  = accuracy_score(y_test_m, y_pred_codes)\n",
    "tka  = top_k_acc(y_test_m, proba_test, classes_model, k=TOP_K)\n",
    "print(f\"\\n[HOLDOUT] Accuracy: {acc:.4f}\")\n",
    "print(f\"[HOLDOUT] Top-{TOP_K} Accuracy: {tka:.4f}\")\n",
    "\n",
    "# -----------------\n",
    "# 6) Salvar artefatos p/ inferência futura\n",
    "# -----------------\n",
    "artifacts = {\n",
    "    \"model\": model,\n",
    "    \"features\": features,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"classes_model\": classes_model,\n",
    "    \"le_dest\": le_dest,\n",
    "    \"le_period\": le_period,\n",
    "    \"le_origin\": le_origin,\n",
    "    \"le_bus\": le_bus,\n",
    "    \"config\": {\n",
    "        \"COVERAGE\": COVERAGE, \"MIN_COUNT\": MIN_COUNT,\n",
    "        \"TRAIN_FRAC\": TRAIN_FRAC, \"TOP_K\": TOP_K\n",
    "    }\n",
    "}\n",
    "joblib.dump(artifacts, os.path.join(ARTS_DIR, \"artifacts.joblib\"))\n",
    "print(f\"[OK] Artefatos salvos em: {ARTS_DIR}/artifacts.joblib\")\n",
    "\n",
    "# -----------------\n",
    "# 7) CSV do HOLDOUT\n",
    "# -----------------\n",
    "ID_COLS = ('order_id','client_id','purchase_datetime','origin_departure','destination_departure')\n",
    "\n",
    "def score_dataframe(model, X_like, df_base_for_ids, id_cols, classes_model, le_dest, topk=3):\n",
    "    import json\n",
    "    proba = model.predict_proba(X_like)\n",
    "    top1_idx   = proba.argmax(axis=1)\n",
    "    top1_codes = classes_model[top1_idx]\n",
    "    top1_lbls  = le_dest.inverse_transform(top1_codes)\n",
    "    top1_prob  = np.round(proba[np.arange(len(proba)), top1_idx] * 100, 2)\n",
    "\n",
    "    k = min(topk, proba.shape[1])\n",
    "    topk_idx     = np.argsort(proba, axis=1)[:, -k:][:, ::-1]\n",
    "    topk_codes   = classes_model[topk_idx]\n",
    "    topk_labels  = [le_dest.inverse_transform(row).tolist() for row in topk_codes]\n",
    "    topk_probs   = [list(np.round(proba[i, topk_idx[i]]*100, 2)) for i in range(len(proba))]\n",
    "\n",
    "    res = df_base_for_ids.loc[X_like.index, list(id_cols)].copy()\n",
    "    res[\"pred_top1\"]        = top1_lbls\n",
    "    res[\"pred_top1_prob\"]   = top1_prob\n",
    "    res[\"pred_topk_labels\"] = [json.dumps(v) for v in topk_labels]\n",
    "    res[\"pred_topk_probs\"]  = [json.dumps(v) for v in topk_probs]\n",
    "    return res.reset_index(drop=True)\n",
    "\n",
    "holdout_preds = score_dataframe(model, X_test, df, ID_COLS, classes_model, le_dest, topk=TOP_K)\n",
    "holdout_preds.to_csv(CSV_HOLDOUT, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] {CSV_HOLDOUT} salvo ({len(holdout_preds):,} linhas)\")\n",
    "\n",
    "# -----------------\n",
    "# 8) CSV da base inteira (em batches para não travar)\n",
    "# -----------------\n",
    "full_X = make_X(df, features, cat_cols)  # já temos as features da base inteira\n",
    "full_preds = score_dataframe_in_batches(\n",
    "    model=model,\n",
    "    X_like=full_X,\n",
    "    df_base_for_ids=df,\n",
    "    id_cols=ID_COLS,\n",
    "    classes_model=classes_model,\n",
    "    le_dest=le_dest,\n",
    "    topk=TOP_K,\n",
    "    batch_size=250_000  # aumente/diminua conforme a RAM\n",
    ")\n",
    "full_preds.to_csv(CSV_FULL, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] {CSV_FULL} salvo ({len(full_preds):,} linhas)\")\n",
    "\n",
    "print(f\"\\n[Done] Tempo total: {time.time() - t0:.1f}s\")\n"
   ],
   "id": "dc97377311254f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Usando df_sample: 870,672 linhas\n",
      "[INFO] Destinos mantidos: 332 | cobertura≈0.900 | '__OUTROS__' aplicado ao restante\n",
      "[INFO] Subamostrando treino: 187,257 linhas\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\tvalid_0's multi_logloss: 32.1666\n",
      "[50]\tvalid_0's multi_logloss: 32.1666\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 6.83479\n",
      "\n",
      "[HOLDOUT] Accuracy: 0.2077\n",
      "[HOLDOUT] Top-3 Accuracy: 0.4723\n",
      "[OK] Artefatos salvos em: artifacts_destino_model/artifacts.joblib\n",
      "[OK] predicoes_holdout.csv salvo (176,010 linhas)\n",
      "[SCORE] 250,000/870,672 linhas processadas...\n",
      "[SCORE] 500,000/870,672 linhas processadas...\n",
      "[SCORE] 750,000/870,672 linhas processadas...\n",
      "[SCORE] 870,672/870,672 linhas processadas...\n",
      "[OK] predicoes_full.csv salvo (870,672 linhas)\n",
      "\n",
      "[Done] Tempo total: 258.6s\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:15:18.729512Z",
     "start_time": "2025-08-19T15:15:17.748760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Baselines (usam apenas o treino!)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X_train_all, y_train_all, X_test, y_test, df, le_dest, model, classes_model já existem do script\n",
    "\n",
    "# A) Baseline global: sempre predizer o destino mais frequente do TREINO\n",
    "major_code = int(pd.Series(y_train_all).value_counts().idxmax())\n",
    "base_global_acc = accuracy_score(y_test, np.full_like(y_test, major_code, dtype=int))\n",
    "print(f\"[BASE] Global (sempre maioria do treino) - Top-1: {base_global_acc:.4f}\")\n",
    "\n",
    "# B) Baseline por origem: predizer, para cada origem, o destino mais frequente no TREINO\n",
    "train_idx = X_train_all.index\n",
    "test_idx  = X_test.index\n",
    "\n",
    "df_train = df.loc[train_idx, ['origin_departure','destination_departure']].copy()\n",
    "df_train['y'] = y_train_all.values\n",
    "# mapeia origem -> destino (código) mais frequente\n",
    "mode_by_origin = (df_train.groupby('origin_departure')['y']\n",
    "                  .agg(lambda s: s.value_counts().index[0]))\n",
    "\n",
    "df_test  = df.loc[test_idx, ['origin_departure']].copy()\n",
    "pred_by_origin = df_test['origin_departure'].map(mode_by_origin).fillna(major_code).astype(int).values\n",
    "base_origin_acc = accuracy_score(y_test, pred_by_origin)\n",
    "print(f\"[BASE] Por origem (moda no treino) - Top-1: {base_origin_acc:.4f}\")\n",
    "\n",
    "# C) Seu modelo (já calculado no script principal)\n",
    "# acc, tka já existem — só reimprime para comparar:\n",
    "print(f\"[MODEL] LightGBM - Top-1: {acc:.4f} | Top-3: {tka:.4f}\")\n"
   ],
   "id": "85357ecf51a6897a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BASE] Global (sempre maioria do treino) - Top-1: 0.1126\n",
      "[BASE] Por origem (moda no treino) - Top-1: 0.3082\n",
      "[MODEL] LightGBM - Top-1: 0.2077 | Top-3: 0.4723\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:07:45.595381Z",
     "start_time": "2025-08-19T16:04:26.455104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# BLENDING com PRIOR P(dest | origem) + CSVs\n",
    "# ============================================\n",
    "import numpy as np, pandas as pd, json, os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "TOP_K = 3\n",
    "OUT_HOLD = \"predicoes_holdout_blend.csv\"\n",
    "OUT_FULL = \"predicoes_full_blend.csv\"\n",
    "\n",
    "# --- helper: top-k (robusto) ---\n",
    "def top_k_acc(y_true, proba, model_classes, k=3):\n",
    "    y_true = np.asarray(y_true)\n",
    "    k = min(k, proba.shape[1])\n",
    "    idx_map = {c:i for i, c in enumerate(model_classes)}\n",
    "    ac = 0\n",
    "    for yt, p in zip(y_true, proba):\n",
    "        if yt in idx_map:\n",
    "            if idx_map[yt] in np.argpartition(p, -k)[-k:]:\n",
    "                ac += 1\n",
    "    return ac / len(y_true) if len(y_true) else np.nan\n",
    "\n",
    "# --- 1) PRIOR por origem usando SOMENTE TREINO ---\n",
    "n_classes = len(classes_model)\n",
    "train_idx = X_train_all.index\n",
    "orig_train = df.loc[train_idx, \"origin_departure\"].astype(str)\n",
    "ytr = y_train_all.loc[train_idx].astype(int)\n",
    "\n",
    "prior_map = {}\n",
    "for orig, idxs in orig_train.groupby(orig_train).groups.items():\n",
    "    yy = ytr.loc[idxs].values\n",
    "    cnt = np.bincount(yy, minlength=n_classes).astype(float)\n",
    "    p = cnt / cnt.sum() if cnt.sum() > 0 else np.full(n_classes, 1.0/n_classes)\n",
    "    prior_map[orig] = p\n",
    "\n",
    "glob_cnt = np.bincount(ytr.values, minlength=n_classes).astype(float)\n",
    "global_prior = glob_cnt / glob_cnt.sum()\n",
    "\n",
    "# --- 2) Grid de alpha para achar melhor mistura no HOLDOUT ---\n",
    "mask_test = y_test.isin(set(classes_model))\n",
    "X_test_m  = X_test[mask_test]\n",
    "y_test_m  = y_test[mask_test]\n",
    "orig_test = df.loc[X_test_m.index, \"origin_departure\"].astype(str).values\n",
    "\n",
    "proba_model = model.predict_proba(X_test_m)\n",
    "prior_mat   = np.vstack([prior_map.get(o, global_prior) for o in orig_test])\n",
    "\n",
    "alphas = [0.0, 0.25, 0.5, 0.75, 1.0]   # pode refinar depois\n",
    "best = {\"alpha\": None, \"acc\": -1, \"topk\": -1}\n",
    "for a in alphas:\n",
    "    blend = a*proba_model + (1-a)*prior_mat\n",
    "    y_pred_codes = classes_model[blend.argmax(axis=1)]\n",
    "    acc  = accuracy_score(y_test_m, y_pred_codes)\n",
    "    tka  = top_k_acc(y_test_m, blend, classes_model, k=TOP_K)\n",
    "    print(f\"[BLEND] alpha={a:.2f} -> Top-1={acc:.4f} | Top-{TOP_K}={tka:.4f}\")\n",
    "    if acc > best[\"acc\"]:\n",
    "        best.update({\"alpha\": a, \"acc\": acc, \"topk\": tka})\n",
    "\n",
    "print(f\"\\n[BLEND] Melhor alpha={best['alpha']:.2f} | Top-1={best['acc']:.4f} | Top-{TOP_K}={best['topk']:.4f}\")\n",
    "\n",
    "# --- 3) CSV do HOLDOUT com BLEND ---\n",
    "blend = best[\"alpha\"]*proba_model + (1-best[\"alpha\"])*prior_mat\n",
    "top1_idx   = blend.argmax(axis=1)\n",
    "top1_codes = classes_model[top1_idx]\n",
    "top1_lbls  = le_dest.inverse_transform(top1_codes)\n",
    "top1_prob  = np.round(blend[np.arange(len(blend)), top1_idx]*100, 2)\n",
    "\n",
    "k = min(TOP_K, blend.shape[1])\n",
    "topk_idx   = np.argsort(blend, axis=1)[:, -k:][:, ::-1]\n",
    "topk_codes = classes_model[topk_idx]\n",
    "topk_lbls  = [le_dest.inverse_transform(row).tolist() for row in topk_codes]\n",
    "topk_probs = [list(np.round(blend[i, topk_idx[i]]*100, 2)) for i in range(len(blend))]\n",
    "\n",
    "hold = df.loc[X_test_m.index, ['order_id','client_id','purchase_datetime','origin_departure','destination_departure']].copy()\n",
    "hold[\"pred_top1_blend\"]        = top1_lbls\n",
    "hold[\"pred_top1_prob_blend\"]   = top1_prob\n",
    "hold[\"pred_topk_labels_blend\"] = [json.dumps(v) for v in topk_lbls]\n",
    "hold[\"pred_topk_probs_blend\"]  = [json.dumps(v) for v in topk_probs]\n",
    "hold.to_csv(OUT_HOLD, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] {OUT_HOLD} salvo ({len(hold):,} linhas)\")\n",
    "\n",
    "# --- 4) CSV da BASE INTEIRA com BLEND (em batches) ---\n",
    "def score_full_blended(model, X_like, df_base, id_cols, classes_model, le_dest, prior_map, global_prior, alpha=0.5, batch=250_000):\n",
    "    out = []\n",
    "    n = len(X_like)\n",
    "    for s in range(0, n, batch):\n",
    "        e = min(s+batch, n)\n",
    "        Xi  = X_like.iloc[s:e]\n",
    "        ids = df_base.loc[Xi.index, list(id_cols)].copy()\n",
    "        proba = model.predict_proba(Xi)\n",
    "        origins = df_base.loc[Xi.index, 'origin_departure'].astype(str).values\n",
    "        prior  = np.vstack([prior_map.get(o, global_prior) for o in origins])\n",
    "        blend  = alpha*proba + (1-alpha)*prior\n",
    "\n",
    "        top1_idx   = blend.argmax(axis=1)\n",
    "        top1_codes = classes_model[top1_idx]\n",
    "        ids[\"pred_top1_blend\"]      = le_dest.inverse_transform(top1_codes)\n",
    "        ids[\"pred_top1_prob_blend\"] = np.round(blend[np.arange(len(blend)), top1_idx]*100, 2)\n",
    "\n",
    "        k = min(TOP_K, blend.shape[1])\n",
    "        topk_idx   = np.argsort(blend, axis=1)[:, -k:][:, ::-1]\n",
    "        topk_codes = classes_model[topk_idx]\n",
    "        topk_lbls  = [le_dest.inverse_transform(row).tolist() for row in topk_codes]\n",
    "        topk_probs = [list(np.round(blend[i, topk_idx[i]]*100, 2)) for i in range(len(blend))]\n",
    "        ids[\"pred_topk_labels_blend\"] = [json.dumps(v) for v in topk_lbls]\n",
    "        ids[\"pred_topk_probs_blend\"]  = [json.dumps(v) for v in topk_probs]\n",
    "\n",
    "        out.append(ids.reset_index(drop=True))\n",
    "        print(f\"[SCORE BLEND] {e:,}/{n:,}\")\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "ID_COLS = ('order_id','client_id','purchase_datetime','origin_departure','destination_departure')\n",
    "\n",
    "# reaproveita as features da base inteira (full_X foi montado no treino)\n",
    "full_X = X_all  # no seu script ele já existe com o mesmo nome\n",
    "full_preds_blend = score_full_blended(\n",
    "    model=model,\n",
    "    X_like=full_X,\n",
    "    df_base=df,\n",
    "    id_cols=ID_COLS,\n",
    "    classes_model=classes_model,\n",
    "    le_dest=le_dest,\n",
    "    prior_map=prior_map,\n",
    "    global_prior=global_prior,\n",
    "    alpha=best[\"alpha\"],\n",
    "    batch=250_000\n",
    ")\n",
    "full_preds_blend.to_csv(OUT_FULL, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] {OUT_FULL} salvo ({len(full_preds_blend):,} linhas)\")\n"
   ],
   "id": "b1589dc5855d2515",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEND] alpha=0.00 -> Top-1=0.3083 | Top-3=0.5200\n",
      "[BLEND] alpha=0.25 -> Top-1=0.3156 | Top-3=0.5499\n",
      "[BLEND] alpha=0.50 -> Top-1=0.2405 | Top-3=0.5514\n",
      "[BLEND] alpha=0.75 -> Top-1=0.2209 | Top-3=0.5402\n",
      "[BLEND] alpha=1.00 -> Top-1=0.2077 | Top-3=0.4723\n",
      "\n",
      "[BLEND] Melhor alpha=0.25 | Top-1=0.3156 | Top-3=0.5499\n",
      "[OK] predicoes_holdout_blend.csv salvo (176,010 linhas)\n",
      "[SCORE BLEND] 250,000/870,672\n",
      "[SCORE BLEND] 500,000/870,672\n",
      "[SCORE BLEND] 750,000/870,672\n",
      "[SCORE BLEND] 870,672/870,672\n",
      "[OK] predicoes_full_blend.csv salvo (870,672 linhas)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:09:59.916657Z",
     "start_time": "2025-08-19T16:09:59.032112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Salva prior_map, global_prior e best_alpha dentro do artifacts.joblib\n",
    "import joblib\n",
    "\n",
    "arts = joblib.load(\"artifacts_destino_model/artifacts.joblib\")\n",
    "\n",
    "arts.update({\n",
    "    \"prior_map\": {str(k): v.tolist() for k, v in prior_map.items()},\n",
    "    \"global_prior\": global_prior.tolist(),\n",
    "    \"best_alpha\": float(best[\"alpha\"]),\n",
    "    # garante que as classes também estejam salvas como lista\n",
    "    \"classes_model\": arts.get(\"classes_model\", classes_model).tolist() if hasattr(classes_model, \"tolist\") else list(classes_model),\n",
    "})\n",
    "\n",
    "joblib.dump(arts, \"artifacts_destino_model/artifacts.joblib\")\n",
    "print(\"[OK] prior_map, global_prior, best_alpha gravados em artifacts.joblib\")\n"
   ],
   "id": "6cad705591f9dbd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] prior_map, global_prior, best_alpha gravados em artifacts.joblib\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:31:39.863724Z",
     "start_time": "2025-08-19T16:31:38.057731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Atualizar artifacts.joblib com prior_map / global_prior / best_alpha ===\n",
    "import joblib, pandas as pd, numpy as np\n",
    "\n",
    "art_path = \"artifacts_destino_model/artifacts.joblib\"\n",
    "arts = joblib.load(art_path)\n",
    "\n",
    "need_prior = not all(k in arts for k in (\"prior_map\",\"global_prior\"))\n",
    "need_alpha = \"best_alpha\" not in arts\n",
    "\n",
    "# 1) Se já existirem em memória (variáveis do blend), usa-as\n",
    "try:\n",
    "    prior_map     # type: ignore\n",
    "    global_prior  # type: ignore\n",
    "    have_in_memory = True\n",
    "except NameError:\n",
    "    have_in_memory = False\n",
    "\n",
    "if have_in_memory:\n",
    "    # Garantir tipos serializáveis e alinhamento\n",
    "    classes = np.array(arts[\"classes_model\"])\n",
    "    arts[\"prior_map\"] = {str(k): np.array(v, dtype=float).tolist() for k, v in prior_map.items()}\n",
    "    arts[\"global_prior\"] = np.array(global_prior, dtype=float).tolist()\n",
    "else:\n",
    "    # 2) Reconstruir a partir do predicoes_full_blend.csv\n",
    "    # (usa rótulos verdadeiros e alinha às classes do modelo)\n",
    "    dfp = pd.read_csv(\"predicoes_full_blend.csv\",\n",
    "                      usecols=[\"origin_departure\",\"destination_departure\"])\n",
    "    le_dest = arts[\"le_dest\"]\n",
    "    classes = np.array(arts[\"classes_model\"])\n",
    "\n",
    "    # Mapeia rótulos \"crus\" para o espaço do encoder (raras -> \"__OUTROS__\")\n",
    "    def map_to_training(lbl: str):\n",
    "        return lbl if lbl in le_dest.classes_ else \"__OUTROS__\"\n",
    "\n",
    "    y_mapped = dfp[\"destination_departure\"].astype(str).map(map_to_training)\n",
    "    y_codes  = le_dest.transform(y_mapped)\n",
    "\n",
    "    # posição de cada classe ativa na matriz de probas\n",
    "    pos = {c:i for i, c in enumerate(classes)}\n",
    "\n",
    "    # prior por origem (vetor no mesmo ordenamento de classes)\n",
    "    prior_map_rec = {}\n",
    "    for orig, grp in dfp.assign(yc=y_codes).groupby(\"origin_departure\"):\n",
    "        counts = np.zeros(len(classes), dtype=float)\n",
    "        vals, cnts = np.unique(grp[\"yc\"].values, return_counts=True)\n",
    "        for v, cnt in zip(vals, cnts):\n",
    "            if v in pos: counts[pos[v]] = cnt\n",
    "        s = counts.sum()\n",
    "        prior_map_rec[str(orig)] = (counts/s if s>0 else np.ones(len(classes))/len(classes)).tolist()\n",
    "\n",
    "    # prior global\n",
    "    g_counts = np.zeros(len(classes), dtype=float)\n",
    "    vals, cnts = np.unique(y_codes, return_counts=True)\n",
    "    for v, cnt in zip(vals, cnts):\n",
    "        if v in pos: g_counts[pos[v]] = cnt\n",
    "    g_prior = (g_counts / g_counts.sum()).tolist()\n",
    "\n",
    "    arts[\"prior_map\"]   = prior_map_rec\n",
    "    arts[\"global_prior\"]= g_prior\n",
    "\n",
    "# 3) alpha (se faltar)\n",
    "if need_alpha:\n",
    "    # usa seu melhor alpha conhecido\n",
    "    arts[\"best_alpha\"] = 0.25\n",
    "\n",
    "# 4) Garantir que classes_model está como lista\n",
    "if hasattr(arts.get(\"classes_model\"), \"tolist\"):\n",
    "    arts[\"classes_model\"] = arts[\"classes_model\"].tolist()\n",
    "\n",
    "joblib.dump(arts, art_path)\n",
    "print(\"[OK] artifacts atualizados com prior_map, global_prior e best_alpha\")\n",
    "\n"
   ],
   "id": "f415762cadeea828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] artifacts atualizados com prior_map, global_prior e best_alpha\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:31:50.177945Z",
     "start_time": "2025-08-19T16:31:49.677881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "arts = joblib.load(\"artifacts_destino_model/artifacts.joblib\")\n",
    "print(\"tem prior_map?\", \"prior_map\" in arts, \"| tem global_prior?\", \"global_prior\" in arts, \"| best_alpha:\", arts.get(\"best_alpha\"))\n"
   ],
   "id": "bd793631b57d9462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tem prior_map? True | tem global_prior? True | best_alpha: 0.25\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:35:20.882181Z",
     "start_time": "2025-08-19T16:35:17.040793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# --- 1) Carrega artefatos e predições do holdout ---\n",
    "arts = load(\"artifacts_destino_model/artifacts.joblib\")\n",
    "classes = np.array(arts[\"classes_model\"])\n",
    "le_dest = arts[\"le_dest\"]\n",
    "\n",
    "dfh = pd.read_csv(\"predicoes_holdout_blend.csv\")\n",
    "# colunas esperadas: destination_departure (true), pred_top1_blend, pred_top1_prob_blend, pred_topk_labels_blend, pred_topk_probs_blend\n",
    "assert {\"destination_departure\",\"pred_top1_blend\",\"pred_top1_prob_blend\"}.issubset(dfh.columns)\n",
    "\n",
    "# --- 2) Métricas globais já no arquivo (reconfere) ---\n",
    "y_true = dfh[\"destination_departure\"].astype(str).values\n",
    "y_pred = dfh[\"pred_top1_blend\"].astype(str).values\n",
    "acc = (y_true == y_pred).mean()\n",
    "\n",
    "# Top-3 do arquivo\n",
    "def in_topk(row):\n",
    "    try:\n",
    "        labels = json.loads(row[\"pred_topk_labels_blend\"])\n",
    "        return row[\"destination_departure\"] in labels\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "top3 = dfh.apply(in_topk, axis=1).mean()\n",
    "print(f\"[CHECK] Top-1={acc:.4f} | Top-3={top3:.4f}\")\n",
    "\n",
    "# --- 3) Calibração do Top-1 ---\n",
    "# binning da confiança do top1 vs acerto real\n",
    "bins = np.linspace(0, 1, 11)  # 10 bins\n",
    "dfh[\"conf\"] = dfh[\"pred_top1_prob_blend\"].astype(float)/100.0\n",
    "dfh[\"ok\"] = (dfh[\"destination_departure\"] == dfh[\"pred_top1_blend\"]).astype(int)\n",
    "dfh[\"bin\"] = pd.cut(dfh[\"conf\"], bins, include_lowest=True)\n",
    "\n",
    "calib = dfh.groupby(\"bin\").agg(\n",
    "    n=(\"ok\",\"size\"),\n",
    "    conf_m=(\"conf\",\"mean\"),\n",
    "    acc_m=(\"ok\",\"mean\")\n",
    ").reset_index()\n",
    "calib[\"gap\"] = calib[\"acc_m\"] - calib[\"conf_m\"]\n",
    "ece = (calib[\"n\"] * calib[\"gap\"].abs()).sum() / calib[\"n\"].sum()\n",
    "print(f\"[CALIB] ECE (Top-1) = {ece:.4f}\")\n",
    "print(calib[[\"bin\",\"n\",\"conf_m\",\"acc_m\",\"gap\"]])\n",
    "\n",
    "# --- 4) Limiar de confiança ---\n",
    "for tau in [0.10, 0.15, 0.20, 0.25]:\n",
    "    mask = dfh[\"conf\"] >= tau\n",
    "    cov = mask.mean()\n",
    "    acc_cond = dfh.loc[mask, \"ok\"].mean() if cov > 0 else np.nan\n",
    "    print(f\"[THRESH] τ={tau:.2f} -> cobertura={cov:.3f} | acc@conf≥τ={acc_cond:.3f}\")\n",
    "\n",
    "# --- 5) Métricas por segmentos (origem e mês/hora se existirem) ---\n",
    "seg_cols = []\n",
    "if \"origin_departure\" in dfh.columns: seg_cols.append(\"origin_departure\")\n",
    "if \"purchase_datetime\" in dfh.columns:\n",
    "    ts = pd.to_datetime(dfh[\"purchase_datetime\"], errors=\"coerce\")\n",
    "    dfh[\"month\"] = ts.dt.month\n",
    "    dfh[\"hour\"]  = ts.dt.hour\n",
    "    seg_cols += [\"month\",\"hour\"]\n",
    "\n",
    "for col in seg_cols:\n",
    "    g = dfh.groupby(col)[\"ok\"].agg([\"size\",\"mean\"]).sort_values(\"size\", ascending=False).head(15)\n",
    "    print(f\"\\n[SEG] Top 15 {col} por volume:\")\n",
    "    print(g.rename(columns={\"size\":\"n\",\"mean\":\"acc\"}))\n",
    "\n",
    "# --- 6) Head vs Tail por frequência do destino (no TREINO) ---\n",
    "# Reconstrói a frequência de classes do TREINO a partir do artifacts (se você salvou y_train_all, pule isso; aqui usamos prior global como proxy)\n",
    "# Melhor: se você tiver um CSV com destino do TREINO, use-o. Como fallback, usamos o prior_global para ordenar classes (proxy).\n",
    "prior = np.array(arts[\"global_prior\"], dtype=float)\n",
    "order = np.argsort(prior)[::-1]  # classes mais frequentes primeiro (aproximação)\n",
    "# mapeia rótulo -> rank aproximado de frequência\n",
    "rank = {le_dest.inverse_transform([c])[0]: i for i, c in enumerate(classes[order])}\n",
    "\n",
    "dfh[\"freq_rank\"] = dfh[\"destination_departure\"].map(rank).fillna(len(rank)+1).astype(int)\n",
    "# buckets (head <= 10%, mid 10-50%, tail > 50%)\n",
    "q1, q5 = np.percentile(list(rank.values()), [10, 50])\n",
    "def bucket(r):\n",
    "    if r <= q1: return \"head\"\n",
    "    if r <= q5: return \"mid\"\n",
    "    return \"tail\"\n",
    "dfh[\"bucket\"] = dfh[\"freq_rank\"].apply(bucket)\n",
    "print(\"\\n[HEAD/TAIL] acc por bucket:\")\n",
    "print(dfh.groupby(\"bucket\")[\"ok\"].agg([\"size\",\"mean\"]).rename(columns={\"size\":\"n\",\"mean\":\"acc\"}))\n",
    "\n",
    "# --- 7) Drift simples (PSI) treino vs holdout em features numéricas se você tiver X_train_all/X_test salvos ---\n",
    "# Se não tiver, pule. Exemplo de função PSI para uma coluna:\n",
    "def psi(expected, actual, bins=10):\n",
    "    e_bins = pd.qcut(expected, q=bins, duplicates='drop')\n",
    "    a_bins = pd.cut(actual,   pd.IntervalIndex(e_bins.cat.categories))\n",
    "    e_pct = e_bins.value_counts(normalize=True).sort_index()\n",
    "    a_pct = a_bins.value_counts(normalize=True).reindex(e_pct.index).fillna(0)\n",
    "    return ((a_pct - e_pct) * np.log((a_pct + 1e-9) / (e_pct + 1e-9))).sum()\n",
    "\n",
    "# Exemplo (se você tiver as colunas brutas no holdout):\n",
    "num_cols = [c for c in [\"total_value\",\"tickets_quantity\",\"days_since_last\",\"client_total_orders\",\"client_avg_value\"] if c in dfh.columns]\n",
    "if num_cols:\n",
    "    print(\"\\n[DRIFT] PSI (usar treino real como 'expected' quando disponível):\")\n",
    "    # aqui só ilustramos PSI do holdout c/ ele mesmo (não faz sentido estatístico, apenas placeholder)\n",
    "    for c in num_cols:\n",
    "        v = dfh[c].dropna()\n",
    "        if len(v) > 100:\n",
    "            print(f\"{c}: PSI≈{psi(v.sample(min(5000,len(v)), random_state=42), v.sample(min(5000,len(v)), random_state=43)):.4f} (placeholder)\")\n"
   ],
   "id": "4382a4d9623e3bde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Top-1=0.0000 | Top-3=0.0000\n",
      "[CALIB] ECE (Top-1) = 0.2845\n",
      "             bin      n    conf_m  acc_m       gap\n",
      "0  (-0.001, 0.1]     20  0.093415    0.0 -0.093415\n",
      "1     (0.1, 0.2]  48929  0.159785    0.0 -0.159785\n",
      "2     (0.2, 0.3]  81655  0.247037    0.0 -0.247037\n",
      "3     (0.3, 0.4]  20329  0.342816    0.0 -0.342816\n",
      "4     (0.4, 0.5]   9346  0.449467    0.0 -0.449467\n",
      "5     (0.5, 0.6]   4261  0.545833    0.0 -0.545833\n",
      "6     (0.6, 0.7]   4659  0.648238    0.0 -0.648238\n",
      "7     (0.7, 0.8]   4198  0.744091    0.0 -0.744091\n",
      "8     (0.8, 0.9]   1028  0.850412    0.0 -0.850412\n",
      "9     (0.9, 1.0]   1585  0.987860    0.0 -0.987860\n",
      "[THRESH] τ=0.10 -> cobertura=1.000 | acc@conf≥τ=0.000\n",
      "[THRESH] τ=0.15 -> cobertura=0.894 | acc@conf≥τ=0.000\n",
      "[THRESH] τ=0.20 -> cobertura=0.722 | acc@conf≥τ=0.000\n",
      "[THRESH] τ=0.25 -> cobertura=0.501 | acc@conf≥τ=0.000\n",
      "\n",
      "[SEG] Top 15 origin_departure por volume:\n",
      "                                                        n  acc\n",
      "origin_departure                                              \n",
      "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d4...  26074  0.0\n",
      "7688b6ef52555962d008fff894223582c484517cea7da49...  16053  0.0\n",
      "2fca346db656187102ce806ac732e06a62df0dbb2829e51...   7402  0.0\n",
      "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01...   6350  0.0\n",
      "fbb2a73b0bacf3953186a92029e3e9b130373a9ff144940...   5389  0.0\n",
      "81b8a03f97e8787c53fe1a86bda042b6f0de9b0ec9c0935...   5352  0.0\n",
      "48449a14a4ff7d79bb7a1b6f3d488eba397c36ef25634c1...   3501  0.0\n",
      "4652614c4d8778e57a970722f142d832798c133a14b1232...   3166  0.0\n",
      "f369cb89fc627e668987007d121ed1eacdc01db9e28f8bb...   2876  0.0\n",
      "eb624dbe56eb6620ae62080c10a273cab73ae8eca98ab17...   2711  0.0\n",
      "62f77e7d6197863ac98d9e0cfa76bea0c8e05379ed5281a...   2696  0.0\n",
      "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f9...   2630  0.0\n",
      "3ada92f28b4ceda38562ebf047c6ff05400d4c572352a11...   2525  0.0\n",
      "ef2d127de37b942baad06145e54b0c619a1f22327b2ebbc...   2109  0.0\n",
      "d26eae87829adde551bf4b852f9da6b8c3c2db9b65b8b68...   1966  0.0\n",
      "\n",
      "[SEG] Top 15 month por volume:\n",
      "           n  acc\n",
      "month            \n",
      "12     23407  0.0\n",
      "1      18581  0.0\n",
      "2      15974  0.0\n",
      "11     15641  0.0\n",
      "3      14721  0.0\n",
      "10     14692  0.0\n",
      "7      13298  0.0\n",
      "9      12892  0.0\n",
      "8      12450  0.0\n",
      "4      11635  0.0\n",
      "6      11554  0.0\n",
      "5      11165  0.0\n",
      "\n",
      "[SEG] Top 15 hour por volume:\n",
      "          n  acc\n",
      "hour            \n",
      "11    11943  0.0\n",
      "13    11786  0.0\n",
      "14    11678  0.0\n",
      "12    11624  0.0\n",
      "15    11236  0.0\n",
      "19    11088  0.0\n",
      "10    11079  0.0\n",
      "18    10955  0.0\n",
      "16    10871  0.0\n",
      "17    10736  0.0\n",
      "20    10207  0.0\n",
      "21     9383  0.0\n",
      "9      9196  0.0\n",
      "22     8115  0.0\n",
      "8      6702  0.0\n",
      "\n",
      "[HEAD/TAIL] acc por bucket:\n",
      "             n  acc\n",
      "bucket             \n",
      "tail    176010  0.0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:39:57.149953Z",
     "start_time": "2025-08-19T16:39:21.621165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Avaliação robusta do holdout (corrige mismatch de rótulos) ===\n",
    "import json, ast, numpy as np, pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "ART = \"artifacts_destino_model/artifacts.joblib\"\n",
    "CSV = \"predicoes_holdout_blend.csv\"\n",
    "\n",
    "arts = load(ART)\n",
    "le_dest = arts[\"le_dest\"]\n",
    "classes_codes = np.array(arts[\"classes_model\"])  # códigos (inteiros) no mesmo eixo do predict_proba\n",
    "classes_labels = le_dest.inverse_transform(classes_codes)  # nomes (strings) correspondentes\n",
    "\n",
    "# Mapeadores úteis\n",
    "code2label = {int(c): l for c, l in zip(classes_codes, classes_labels)}\n",
    "labels_set = set(le_dest.classes_)  # universo de labels (strings) visto no treino (inclui \"__OUTROS__\")\n",
    "\n",
    "dfh = pd.read_csv(CSV)\n",
    "\n",
    "# --- Funções auxiliares -------------------------------------------------------\n",
    "def normalize_true_labels(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coloca y_true no mesmo espaço do treino: se não visto -> '__OUTROS__'.\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    mask_known = s.isin(labels_set)\n",
    "    s = s.where(mask_known, \"__OUTROS__\")\n",
    "    return s\n",
    "\n",
    "def looks_numeric_series(s: pd.Series, thresh: float = 0.8) -> bool:\n",
    "    \"\"\"Retorna True se >= thresh dos valores forem parseáveis como inteiros.\"\"\"\n",
    "    sn = pd.to_numeric(s, errors=\"coerce\")\n",
    "    ratio = (~sn.isna()).mean()\n",
    "    return ratio >= thresh\n",
    "\n",
    "def decode_pred_series_to_labels(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converte a coluna de predição Top-1 para labels (strings).\n",
    "    - Se já são labels do treino, retorna como está.\n",
    "    - Se parecem numéricas, faz mapping code->label.\n",
    "    - Caso contrário, retorna strings limpas.\n",
    "    \"\"\"\n",
    "    s_clean = s.astype(str).str.strip()\n",
    "    if s_clean.isin(labels_set).mean() > 0.9:\n",
    "        return s_clean\n",
    "    if looks_numeric_series(s_clean):\n",
    "        sn = pd.to_numeric(s_clean, errors=\"coerce\").astype(\"Int64\")\n",
    "        return sn.map(code2label).fillna(\"__OUTROS__\")\n",
    "    return s_clean  # melhor esforço\n",
    "\n",
    "def parse_labels_list(cell):\n",
    "    \"\"\"\n",
    "    Lê a célula de Top-k:\n",
    "    - Tenta json.loads\n",
    "    - fallback: ast.literal_eval\n",
    "    - Converte códigos -> labels se necessário\n",
    "    - Retorna lista de strings\n",
    "    \"\"\"\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    txt = str(cell).strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "    lst = None\n",
    "    try:\n",
    "        lst = json.loads(txt)\n",
    "    except Exception:\n",
    "        try:\n",
    "            lst = ast.literal_eval(txt)\n",
    "        except Exception:\n",
    "            return []\n",
    "    if not isinstance(lst, (list, tuple)):\n",
    "        return []\n",
    "\n",
    "    # Se for lista de números (ou strings numéricas), mapeia para labels\n",
    "    arr = pd.Series(lst)\n",
    "    if looks_numeric_series(arr, thresh=1.0):  # todos numéricos\n",
    "        arr = pd.to_numeric(arr, errors=\"coerce\").astype(\"Int64\")\n",
    "        return [code2label.get(int(x), \"__OUTROS__\") for x in arr.dropna().tolist()]\n",
    "    # Se já são strings, padroniza e, se não pertencem ao universo, mantém assim (pode vir label hash já correto)\n",
    "    return [str(x).strip() for x in arr.tolist()]\n",
    "\n",
    "# --- Normalização das colunas relevantes --------------------------------------\n",
    "assert \"destination_departure\" in dfh.columns, \"CSV precisa ter destination_departure\"\n",
    "assert \"pred_top1_blend\" in dfh.columns, \"CSV precisa ter pred_top1_blend\"\n",
    "\n",
    "y_true = normalize_true_labels(dfh[\"destination_departure\"])\n",
    "y_pred_top1 = decode_pred_series_to_labels(dfh[\"pred_top1_blend\"])\n",
    "\n",
    "# Top-3 (se existir no CSV)\n",
    "has_topk = \"pred_topk_labels_blend\" in dfh.columns\n",
    "if has_topk:\n",
    "    topk_lists = dfh[\"pred_topk_labels_blend\"].apply(parse_labels_list)\n",
    "else:\n",
    "    topk_lists = pd.Series([[]]*len(dfh))\n",
    "\n",
    "# --- Métricas globais ---------------------------------------------------------\n",
    "acc = (y_true == y_pred_top1).mean()\n",
    "if has_topk:\n",
    "    top3 = (pd.Series([yt in lst for yt, lst in zip(y_true, topk_lists)])).mean()\n",
    "else:\n",
    "    top3 = np.nan\n",
    "\n",
    "print(f\"[RESULT] Top-1={acc:.4f} | Top-3={0 if np.isnan(top3) else top3:.4f}\")\n",
    "\n",
    "# --- Calibração (se existir probabilidade do top1) ----------------------------\n",
    "ece = np.nan\n",
    "calib_df = None\n",
    "if \"pred_top1_prob_blend\" in dfh.columns:\n",
    "    conf = pd.to_numeric(dfh[\"pred_top1_prob_blend\"], errors=\"coerce\")/100.0\n",
    "    ok = (y_true == y_pred_top1).astype(int)\n",
    "    bins = np.linspace(0,1,11)\n",
    "    binned = pd.cut(conf, bins, include_lowest=True)\n",
    "    calib_df = pd.DataFrame({\n",
    "        \"n\": ok.groupby(binned).size(),\n",
    "        \"conf_m\": conf.groupby(binned).mean(),\n",
    "        \"acc_m\": ok.groupby(binned).mean()\n",
    "    }).reset_index().rename(columns={\"index\":\"bin\"})\n",
    "    calib_df[\"gap\"] = calib_df[\"acc_m\"] - calib_df[\"conf_m\"]\n",
    "    ece = (calib_df[\"n\"] * calib_df[\"gap\"].abs()).sum() / calib_df[\"n\"].sum()\n",
    "    print(f\"[CALIB] ECE={ece:.4f}\")\n",
    "\n",
    "# --- Segmentos (se colunas existirem) -----------------------------------------\n",
    "seg_reports = {}\n",
    "df_eval = pd.DataFrame({\n",
    "    \"y_true\": y_true,\n",
    "    \"y_pred\": y_pred_top1\n",
    "})\n",
    "# anexar colunas que existirem\n",
    "for col in (\"origin_departure\",\"purchase_datetime\"):\n",
    "    if col in dfh.columns:\n",
    "        df_eval[col] = dfh[col]\n",
    "\n",
    "if \"purchase_datetime\" in df_eval.columns:\n",
    "    ts = pd.to_datetime(df_eval[\"purchase_datetime\"], errors=\"coerce\")\n",
    "    df_eval[\"month\"] = ts.dt.month\n",
    "    df_eval[\"hour\"]  = ts.dt.hour\n",
    "\n",
    "df_eval[\"ok\"] = (df_eval[\"y_true\"] == df_eval[\"y_pred\"]).astype(int)\n",
    "\n",
    "for seg in [c for c in [\"origin_departure\",\"month\",\"hour\"] if c in df_eval.columns]:\n",
    "    g = df_eval.groupby(seg)[\"ok\"].agg(n=\"size\", acc=\"mean\").reset_index().sort_values(\"n\", ascending=False)\n",
    "    seg_reports[seg] = g\n",
    "\n",
    "# --- Salvar relatórios --------------------------------------------------------\n",
    "out_dir = \"eval_reports\"\n",
    "import os; os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"metric\":[\"top1\",\"top3\",\"ece\"],\n",
    "    \"value\":[acc, float(top3) if not np.isnan(top3) else np.nan, ece]\n",
    "}).to_csv(f\"{out_dir}/summary_metrics.csv\", index=False)\n",
    "\n",
    "if calib_df is not None:\n",
    "    calib_df.to_csv(f\"{out_dir}/calibration_bins.csv\", index=False)\n",
    "\n",
    "for seg, g in seg_reports.items():\n",
    "    g.to_csv(f\"{out_dir}/segment_{seg}.csv\", index=False)\n",
    "\n",
    "print(\"[OK] Relatórios salvos em:\", out_dir)\n"
   ],
   "id": "bcdb428f882a09e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Top-1=0.0605 | Top-3=0.4995\n",
      "[CALIB] ECE=0.2240\n",
      "[OK] Relatórios salvos em: eval_reports\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:43:06.428001Z",
     "start_time": "2025-08-19T16:43:00.819019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json, ast, pandas as pd\n",
    "\n",
    "def parse_list(cell):\n",
    "    try:\n",
    "        return json.loads(cell)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(str(cell))\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "dfh = pd.read_csv(\"predicoes_holdout_blend.csv\")\n",
    "\n",
    "# Top-k como lista\n",
    "topk = dfh[\"pred_topk_labels_blend\"].apply(parse_list)\n",
    "\n",
    "# Checa concordância entre a coluna 'pred_top1_blend' e o 1º item da lista Top-k\n",
    "agree = (dfh[\"pred_top1_blend\"].astype(str)\n",
    "         == topk.apply(lambda lst: str(lst[0]) if lst else \"\")).mean()\n",
    "print(f\"[CHECK] agreement pred_top1_blend vs topk[0]: {agree:.3f}\")\n",
    "\n",
    "# Se a concordância for < 0.9, vamos usar o topk[0] como top-1 \"correto\"\n",
    "use_fixed_top1 = agree < 0.90\n",
    "dfh[\"pred_top1_blend_fixed\"] = topk.apply(lambda lst: str(lst[0]) if lst else \"\")\n",
    "\n",
    "# Recalcular métricas com a coluna \"fixa\"\n",
    "y_true = dfh[\"destination_departure\"].astype(str)\n",
    "top1_fixed = (y_true == dfh[\"pred_top1_blend_fixed\"]).mean()\n",
    "top3 = dfh.apply(lambda r: r[\"destination_departure\"] in (parse_list(r[\"pred_topk_labels_blend\"]) or []), axis=1).mean()\n",
    "\n",
    "print(f\"[RESULT] Top-1(recalc)= {top1_fixed:.4f} | Top-3= {top3:.4f}\")\n",
    "\n",
    "# (Opcional) salvar um CSV \"corrigido\" para evitar novos mismatches na equipe\n",
    "dfh.to_csv(\"predicoes_holdout_blend_fixed.csv\", index=False)\n",
    "print(\"[OK] predicoes_holdout_blend_fixed.csv salvo\")\n"
   ],
   "id": "b4a1c5a855ac19fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] agreement pred_top1_blend vs topk[0]: 1.000\n",
      "[RESULT] Top-1(recalc)= 0.0000 | Top-3= 0.0000\n",
      "[OK] predicoes_holdout_blend_fixed.csv salvo\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:46:01.130959Z",
     "start_time": "2025-08-19T16:45:56.853121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Avaliação ALINHADA ao espaço de classes do modelo (códigos) ===\n",
    "import json, ast, numpy as np, pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "ART = \"artifacts_destino_model/artifacts.joblib\"\n",
    "CSV = \"predicoes_holdout_blend.csv\"\n",
    "\n",
    "arts = load(ART)\n",
    "le_dest = arts[\"le_dest\"]                      # LabelEncoder treinado do destino\n",
    "classes_codes = np.array(arts[\"classes_model\"])  # códigos inteiros usados no modelo\n",
    "classes_labels = le_dest.inverse_transform(classes_codes)\n",
    "\n",
    "# Mapas úteis\n",
    "label2code = {lab: int(code) for lab, code in zip(classes_labels, classes_codes)}\n",
    "code2label = {int(code): lab for lab, code in label2code.items()}\n",
    "HAS_OUTROS = \"__OUTROS__\" in le_dest.classes_\n",
    "OUTROS_CODE = label2code[\"__OUTROS__\"] if HAS_OUTROS else None\n",
    "\n",
    "def parse_list(cell):\n",
    "    if pd.isna(cell): return []\n",
    "    s = str(cell).strip()\n",
    "    if not s: return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def series_looks_numeric(s: pd.Series, thresh=0.8) -> bool:\n",
    "    sn = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return (~sn.isna()).mean() >= thresh\n",
    "\n",
    "def to_codes_from_mixed(series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converte uma série de rótulos que podem estar como número (código) ou string (hash)\n",
    "    para códigos do modelo. Valores fora do universo vão para __OUTROS__ (se existir).\n",
    "    \"\"\"\n",
    "    s = series.copy()\n",
    "    if series_looks_numeric(s):\n",
    "        # Já parecem códigos\n",
    "        sn = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "        if OUTROS_CODE is not None:\n",
    "            sn = sn.where(~sn.isna(), OUTROS_CODE)\n",
    "        return sn.astype(int)\n",
    "    else:\n",
    "        # Strings -> códigos via label2code\n",
    "        s = s.astype(str).str.strip()\n",
    "        if OUTROS_CODE is not None:\n",
    "            return s.map(label2code).fillna(OUTROS_CODE).astype(int)\n",
    "        else:\n",
    "            # Se não houver __OUTROS__, dropamos linhas desconhecidas\n",
    "            known = s.isin(label2code)\n",
    "            print(f\"[WARN] Sem __OUTROS__. Removendo {(~known).sum()} linhas com rótulos desconhecidos.\")\n",
    "            return s[known].map(label2code).astype(int)\n",
    "\n",
    "def list_to_codes(lst):\n",
    "    if not isinstance(lst, (list, tuple)): return []\n",
    "    if len(lst) == 0: return []\n",
    "    # Se forem números -> códigos direto\n",
    "    all_num = True\n",
    "    out = []\n",
    "    for x in lst:\n",
    "        try:\n",
    "            xi = int(x)\n",
    "            out.append(xi)\n",
    "        except Exception:\n",
    "            all_num = False\n",
    "            break\n",
    "    if all_num:\n",
    "        return out\n",
    "    # Se forem strings -> map\n",
    "    mapped = []\n",
    "    for x in lst:\n",
    "        lab = str(x).strip()\n",
    "        if lab in label2code:\n",
    "            mapped.append(label2code[lab])\n",
    "        elif OUTROS_CODE is not None:\n",
    "            mapped.append(OUTROS_CODE)\n",
    "    return mapped\n",
    "\n",
    "# -------------------- Carrega CSV e normaliza --------------------\n",
    "dfh = pd.read_csv(CSV)\n",
    "\n",
    "assert \"destination_departure\" in dfh.columns, \"CSV precisa de 'destination_departure'\"\n",
    "assert \"pred_top1_blend\" in dfh.columns, \"CSV precisa de 'pred_top1_blend'\"\n",
    "\n",
    "y_true_codes  = to_codes_from_mixed(dfh[\"destination_departure\"])\n",
    "y_pred1_codes = to_codes_from_mixed(dfh[\"pred_top1_blend\"])\n",
    "\n",
    "if \"pred_topk_labels_blend\" in dfh.columns:\n",
    "    topk_codes = dfh[\"pred_topk_labels_blend\"].apply(parse_list).apply(list_to_codes)\n",
    "else:\n",
    "    topk_codes = pd.Series([[]]*len(dfh))\n",
    "\n",
    "# Remove linhas inválidas (se aparecer algo NaN após conversão)\n",
    "mask_valid = (~pd.isna(y_true_codes)) & (~pd.isna(y_pred1_codes))\n",
    "n_drop = (~mask_valid).sum()\n",
    "if n_drop > 0:\n",
    "    print(f\"[INFO] Removendo {n_drop} linhas inválidas após alinhamento.\")\n",
    "y_true_codes  = y_true_codes[mask_valid].astype(int)\n",
    "y_pred1_codes = y_pred1_codes[mask_valid].astype(int)\n",
    "topk_codes    = topk_codes[mask_valid].reset_index(drop=True)\n",
    "\n",
    "# -------------------- Métricas --------------------\n",
    "acc = (y_true_codes.values == y_pred1_codes.values).mean()\n",
    "# Top-3\n",
    "in_top3 = []\n",
    "for yt, lst in zip(y_true_codes.values, topk_codes.values):\n",
    "    in_top3.append(yt in (lst[:3] if lst else []))\n",
    "top3 = np.mean(in_top3) if len(in_top3) else np.nan\n",
    "\n",
    "print(f\"[ALIGNED] Top-1={acc:.4f} | Top-3={0.0 if np.isnan(top3) else top3:.4f}\")\n",
    "\n",
    "# -------------------- (Opcional) Calibração do Top-1 --------------------\n",
    "ece = np.nan\n",
    "if \"pred_top1_prob_blend\" in dfh.columns:\n",
    "    conf = pd.to_numeric(dfh.loc[mask_valid, \"pred_top1_prob_blend\"], errors=\"coerce\")/100.0\n",
    "    ok = (y_true_codes.values == y_pred1_codes.values).astype(int)\n",
    "    bins = np.linspace(0,1,11)\n",
    "    binned = pd.cut(conf, bins, include_lowest=True)\n",
    "    calib = pd.DataFrame({\n",
    "        \"n\": pd.Series(ok).groupby(binned).size(),\n",
    "        \"conf_m\": conf.groupby(binned).mean(),\n",
    "        \"acc_m\": pd.Series(ok).groupby(binned).mean()\n",
    "    }).reset_index().rename(columns={\"index\":\"bin\"})\n",
    "    calib[\"gap\"] = calib[\"acc_m\"] - calib[\"conf_m\"]\n",
    "    ece = (calib[\"n\"] * calib[\"gap\"].abs()).sum() / calib[\"n\"].sum()\n",
    "    print(f\"[CALIB] ECE={ece:.4f}\")\n",
    "\n",
    "# -------------------- Salva CSV ALINHADO --------------------\n",
    "aligned = pd.DataFrame({\n",
    "    \"y_true_code\": y_true_codes.values,\n",
    "    \"y_pred1_code\": y_pred1_codes.values,\n",
    "    \"in_top3\": in_top3\n",
    "})\n",
    "# anexa, para conferência humana, as versões em label do par verdadeiro/predito:\n",
    "aligned[\"y_true_label\"]  = aligned[\"y_true_code\"].map(code2label)\n",
    "aligned[\"y_pred1_label\"] = aligned[\"y_pred1_code\"].map(code2label)\n",
    "\n",
    "outf = \"predicoes_holdout_blend_aligned.csv\"\n",
    "aligned.to_csv(outf, index=False)\n",
    "print(\"[OK] Alinhado salvo em:\", outf)\n"
   ],
   "id": "467c496fd7146ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ALIGNED] Top-1=0.3156 | Top-3=0.5499\n",
      "[CALIB] ECE=0.0439\n",
      "[OK] Alinhado salvo em: predicoes_holdout_blend_aligned.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:51:39.179885Z",
     "start_time": "2025-08-19T16:51:36.846556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.makedirs(\"eval_report\", exist_ok=True)\n",
    "\n",
    "thr_df.to_csv(\"eval_report/threshold_sweep.csv\", index=False)\n",
    "print(\"[OK] eval_report/threshold_sweep.csv salvo\")\n",
    "\n",
    "\n",
    "import json, ast, numpy as np, pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "ART = \"artifacts_destino_model/artifacts.joblib\"\n",
    "CSV = \"predicoes_holdout_blend.csv\"  # contém pred_top1_prob_blend (%)\n",
    "\n",
    "arts = load(ART)\n",
    "le_dest = arts[\"le_dest\"]\n",
    "classes_codes = np.array(arts[\"classes_model\"])\n",
    "classes_labels = le_dest.inverse_transform(classes_codes)\n",
    "label2code = {lab:int(code) for lab,code in zip(classes_labels, classes_codes)}\n",
    "HAS_OUTROS = \"__OUTROS__\" in le_dest.classes_\n",
    "OUTROS_CODE = label2code[\"__OUTROS__\"] if HAS_OUTROS else None\n",
    "\n",
    "def series_looks_numeric(s, t=0.8): return (~pd.to_numeric(s, errors=\"coerce\").isna()).mean()>=t\n",
    "def to_codes_from_mixed(s):\n",
    "    if series_looks_numeric(s):\n",
    "        sn = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "        return sn.where(~sn.isna(), OUTROS_CODE).astype(int)\n",
    "    s = s.astype(str).str.strip()\n",
    "    return s.map(label2code).fillna(OUTROS_CODE).astype(int)\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "y_true = to_codes_from_mixed(df[\"destination_departure\"])\n",
    "y_pred = to_codes_from_mixed(df[\"pred_top1_blend\"])\n",
    "conf   = pd.to_numeric(df[\"pred_top1_prob_blend\"], errors=\"coerce\")/100.0\n",
    "\n",
    "mask = (~y_true.isna()) & (~y_pred.isna()) & (~conf.isna())\n",
    "y_true = y_true[mask].values\n",
    "y_pred = y_pred[mask].values\n",
    "conf   = conf[mask].values\n",
    "\n",
    "taus = np.round(np.linspace(0.05, 0.9, 18), 2)\n",
    "rows = []\n",
    "for t in taus:\n",
    "    sel = conf >= t\n",
    "    cov = sel.mean()                       # fração aceita (cobertura)\n",
    "    acc = (y_true[sel] == y_pred[sel]).mean() if sel.any() else np.nan\n",
    "    rows.append({\"tau\":float(t), \"coverage\":float(cov), \"acc_given_tau\":float(acc)})\n",
    "\n",
    "thr_df = pd.DataFrame(rows)\n",
    "thr_df.to_csv(\"eval_report/threshold_sweep.csv\", index=False)\n",
    "print(thr_df.head(10))\n",
    "print(\"[OK] eval_report/threshold_sweep.csv salvo\")\n"
   ],
   "id": "acabd9cc727c1480",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] eval_report/threshold_sweep.csv salvo\n",
      "    tau  coverage  acc_given_tau\n",
      "0  0.05  1.000000       0.315596\n",
      "1  0.10  0.999886       0.315597\n",
      "2  0.15  0.893756       0.337804\n",
      "3  0.20  0.722266       0.363891\n",
      "4  0.25  0.501108       0.434615\n",
      "5  0.30  0.258275       0.598759\n",
      "6  0.35  0.190432       0.671699\n",
      "7  0.40  0.142566       0.731200\n",
      "8  0.45  0.115050       0.763654\n",
      "9  0.50  0.089654       0.820722\n",
      "[OK] eval_report/threshold_sweep.csv salvo\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:55:05.741551Z",
     "start_time": "2025-08-19T16:53:04.405618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, json, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# ---------- Config ----------\n",
    "ARTS_PATH = \"artifacts_destino_model/artifacts.joblib\"\n",
    "INPUT_CSV = \"predicoes_full_blend.csv\"          # ou \"predicoes_holdout_blend.csv\"\n",
    "OUTPUT_CSV = \"predicoes_full_blend_thresholded.csv\"\n",
    "TAU_ACCEPT = 0.35   # aceita previsão do modelo se conf >= 0.35\n",
    "TAU_PRIOR  = 0.20   # se 0.20 <= conf < 0.35 usa prior por origem; abaixo disso: abstém\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "def parse_list(cell):\n",
    "    if pd.isna(cell): return []\n",
    "    s = str(cell).strip()\n",
    "    if not s: return []\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "# ---------- Carrega artefatos ----------\n",
    "arts = load(ARTS_PATH)\n",
    "le_dest   = arts[\"le_dest\"]\n",
    "classes   = np.array(arts[\"classes_model\"])          # códigos ativos\n",
    "code2lab  = {int(c): lab for c, lab in zip(classes, le_dest.inverse_transform(classes))}\n",
    "lab2code  = {v:k for k,v in code2lab.items()}\n",
    "\n",
    "prior_map = arts.get(\"prior_map\", {})                # dict: origem_label -> destino_label (moda no treino)\n",
    "global_prior = arts.get(\"global_prior\", None)        # label mais frequente global\n",
    "\n",
    "# ---------- Carrega previsões ----------\n",
    "dfp = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# normaliza prob para [0,1]\n",
    "if \"pred_top1_prob_blend\" not in dfp.columns:\n",
    "    raise ValueError(f\"{INPUT_CSV} não tem coluna 'pred_top1_prob_blend'\")\n",
    "\n",
    "conf = pd.to_numeric(dfp[\"pred_top1_prob_blend\"], errors=\"coerce\")\n",
    "# se vier em %, converte para [0,1]\n",
    "if conf.max() > 1.0:\n",
    "    conf = conf / 100.0\n",
    "\n",
    "# top1 previsto (pode ser label ou código)\n",
    "pred_top1 = dfp[\"pred_top1_blend\"].astype(str)\n",
    "\n",
    "# origem (para fallback)\n",
    "origin_col = \"origin_departure\"\n",
    "has_origin = origin_col in dfp.columns\n",
    "if not has_origin:\n",
    "    print(\"[AVISO] CSV não tem 'origin_departure'. Fallback usará apenas 'global_prior'.\")\n",
    "orig = dfp[origin_col].astype(str) if has_origin else pd.Series([\"__NO_ORIGIN__\"]*len(dfp))\n",
    "\n",
    "# ---------- Função de fallback ----------\n",
    "def choose_fallback(origin_label: str):\n",
    "    # prior por origem -> se não houver, global\n",
    "    if origin_label in prior_map:\n",
    "        return prior_map[origin_label]\n",
    "    return global_prior\n",
    "\n",
    "# ---------- Decisão final ----------\n",
    "# Regra:\n",
    "# conf >= TAU_ACCEPT  -> usa modelo\n",
    "# TAU_PRIOR <= conf < TAU_ACCEPT -> usa prior de origem (ou global)\n",
    "# conf < TAU_PRIOR -> abstém (sem previsão final; você pode marcar como NA)\n",
    "decision = np.where(conf >= TAU_ACCEPT, \"model\",\n",
    "             np.where(conf >= TAU_PRIOR, \"prior\", \"abstain\"))\n",
    "\n",
    "final_label = []\n",
    "final_proba = []\n",
    "for i, dec in enumerate(decision):\n",
    "    if dec == \"model\":\n",
    "        lbl = str(pred_top1.iloc[i])\n",
    "        p   = float(conf.iloc[i])\n",
    "    elif dec == \"prior\":\n",
    "        lbl = choose_fallback(str(orig.iloc[i]))\n",
    "        p   = np.nan   # prob. do prior (opcional: você pode preencher com frequência normalizada)\n",
    "    else:\n",
    "        lbl = np.nan\n",
    "        p   = np.nan\n",
    "    final_label.append(lbl)\n",
    "    final_proba.append(p)\n",
    "\n",
    "out = dfp.copy()\n",
    "out[\"decision\"] = decision\n",
    "out[\"final_pred_label\"] = final_label\n",
    "out[\"final_pred_prob\"]  = final_proba\n",
    "\n",
    "# (opcional) mantenha também top-k do modelo para explicar casos aceitos\n",
    "if \"pred_topk_labels_blend\" in out.columns:\n",
    "    # já está como string/lista no CSV de origem\n",
    "    pass\n",
    "\n",
    "# ---------- Métricas se houver ground truth ----------\n",
    "acc_all = np.nan\n",
    "acc_model_only = np.nan\n",
    "coverage_accept = (decision == \"model\").mean()\n",
    "coverage_accept_or_prior = (decision != \"abstain\").mean()\n",
    "\n",
    "if \"destination_departure\" in out.columns:\n",
    "    y_true = out[\"destination_departure\"].astype(str)\n",
    "\n",
    "    mask_all = ~y_true.isna() & ~pd.isna(out[\"final_pred_label\"])\n",
    "    if mask_all.any():\n",
    "        acc_all = (y_true[mask_all].values == out.loc[mask_all, \"final_pred_label\"].astype(str).values).mean()\n",
    "\n",
    "    mask_model = (decision == \"model\")\n",
    "    if mask_model.any():\n",
    "        acc_model_only = (y_true[mask_model].values == out.loc[mask_model, \"final_pred_label\"].astype(str).values).mean()\n",
    "\n",
    "    print(f\"[METRICS] Cobertura(model)={coverage_accept:.3f} | Cobertura(model+prior)={coverage_accept_or_prior:.3f}\")\n",
    "    print(f\"[METRICS] Acc (model-only)={acc_model_only if not np.isnan(acc_model_only) else 'NA'}\")\n",
    "    print(f\"[METRICS] Acc (aceitos: model/prior)={acc_all if not np.isnan(acc_all) else 'NA'}\")\n",
    "else:\n",
    "    print(f\"[INFO] Sem rótulo verdadeiro no CSV. Cobertura(model)={coverage_accept:.3f} | Cobertura(total aceitos)={coverage_accept_or_prior:.3f}\")\n",
    "\n",
    "# ---------- Salva ----------\n",
    "out.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"[OK] CSV final salvo: {OUTPUT_CSV}\")\n"
   ],
   "id": "fe1b95e55a0baac4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRICS] Cobertura(model)=0.193 | Cobertura(model+prior)=0.722\n",
      "[METRICS] Acc (model-only)=0.0\n",
      "[METRICS] Acc (aceitos: model/prior)=0.0\n",
      "[OK] CSV final salvo: predicoes_full_blend_thresholded.csv\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:00:57.654457Z",
     "start_time": "2025-08-19T16:59:48.754718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# ALINHAMENTO + MÉTRICAS\n",
    "# ----------------------------\n",
    "# coberturas já calculadas:\n",
    "# coverage_accept = (decision == \"model\").mean()\n",
    "# coverage_accept_or_prior = (decision != \"abstain\").mean()\n",
    "\n",
    "acc_all = np.nan\n",
    "acc_model_only = np.nan\n",
    "\n",
    "# Conjunto de labels que o modelo realmente conhece\n",
    "model_labels = set(code2lab.values())  # strings invertidas do encoder (mesmas que o modelo preve)\n",
    "\n",
    "if \"destination_departure\" in out.columns:\n",
    "    # Ground truth bruto em string e sem espaços\n",
    "    y_true_raw = out[\"destination_departure\"].astype(str).str.strip()\n",
    "\n",
    "    # Alinha o ground truth ao espaço de classes do modelo\n",
    "    # Tudo que não estiver no espaço do modelo vira \"__OUTROS__\"\n",
    "    def align_gt(lbl):\n",
    "        return lbl if lbl in model_labels else \"__OUTROS__\"\n",
    "\n",
    "    out[\"y_true_aligned\"] = y_true_raw.apply(align_gt)\n",
    "\n",
    "    # Normaliza a previsão final para string/strip\n",
    "    out[\"final_pred_label\"] = out[\"final_pred_label\"].astype(str).str.strip()\n",
    "\n",
    "    # Métrica nos exemplos aceitos (model OU prior)\n",
    "    m_all = (~out[\"final_pred_label\"].isna()) & (out[\"decision\"] != \"abstain\")\n",
    "    if m_all.any():\n",
    "        acc_all = (out.loc[m_all, \"final_pred_label\"] == out.loc[m_all, \"y_true_aligned\"]).mean()\n",
    "\n",
    "    # Métrica só nos aceitos pelo MODELO (sem prior)\n",
    "    m_model = (out[\"decision\"] == \"model\")\n",
    "    if m_model.any():\n",
    "        acc_model_only = (out.loc[m_model, \"final_pred_label\"] == out.loc[m_model, \"y_true_aligned\"]).mean()\n",
    "\n",
    "    print(f\"[METRICS] Cobertura(model)={coverage_accept:.3f} | Cobertura(model+prior)={coverage_accept_or_prior:.3f}\")\n",
    "    print(f\"[METRICS] Acc (model-only)={acc_model_only if not np.isnan(acc_model_only) else 'NA'}\")\n",
    "    print(f\"[METRICS] Acc (aceitos: model/prior)={acc_all if not np.isnan(acc_all) else 'NA'}\")\n",
    "else:\n",
    "    print(f\"[INFO] CSV sem ground truth ('destination_departure').\")\n",
    "    print(f\"[INFO] Cobertura(model)={coverage_accept:.3f} | Cobertura(total aceitos)={coverage_accept_or_prior:.3f}\")\n",
    "\n",
    "# Salva já com y_true_aligned (útil pra auditoria)\n",
    "out.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"[OK] CSV final salvo: {OUTPUT_CSV}\")\n"
   ],
   "id": "395f2de6c4c0f5f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRICS] Cobertura(model)=0.193 | Cobertura(model+prior)=0.722\n",
      "[METRICS] Acc (model-only)=0.04022967987623468\n",
      "[METRICS] Acc (aceitos: model/prior)=0.010750345836447187\n",
      "[OK] CSV final salvo: predicoes_full_blend_thresholded.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:15:51.654011Z",
     "start_time": "2025-08-19T17:15:34.069643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, json, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# =======================\n",
    "# CONFIG\n",
    "# =======================\n",
    "ARTS_PATH   = \"artifacts_destino_model/artifacts.joblib\"\n",
    "FULL_CSV    = \"predicoes_full_blend.csv\"      # já existente\n",
    "OUT_DIR     = \"eval_report\"\n",
    "\n",
    "MODE        = \"no_prior\"                      # manter simples\n",
    "TAU_ACCEPT  = 0.35                            # threshold de aceitação do modelo\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =======================\n",
    "# Carregar artefatos (para alinhar labels)\n",
    "# =======================\n",
    "arts = load(ARTS_PATH)\n",
    "le_dest      = arts[\"le_dest\"]\n",
    "classes      = np.array(arts[\"classes_model\"])\n",
    "model_labels = set(le_dest.inverse_transform(classes))\n",
    "\n",
    "def align_label(lbl: str) -> str:\n",
    "    return lbl if lbl in model_labels else \"__OUTROS__\"\n",
    "\n",
    "# =======================\n",
    "# 1) Ler o FULL e padronizar colunas\n",
    "# =======================\n",
    "df = pd.read_csv(FULL_CSV)\n",
    "\n",
    "# Predição top-1 — nomes candidatos (o seu já tem 'pred_top1_blend')\n",
    "PRED_CANDIDATES = [\n",
    "    \"pred_top1_blend\",\"pred_top1\",\"pred_label\",\"prediction\",\"pred\",\n",
    "    \"pred_blend_top1\",\"pred_top1_model\",\"pred_model_top1\"\n",
    "]\n",
    "pred_col = next((c for c in PRED_CANDIDATES if c in df.columns), None)\n",
    "if pred_col is None:\n",
    "    raise ValueError(f\"Não achei a coluna de predição top-1. Procurei: {PRED_CANDIDATES}.\")\n",
    "df[\"pred_top1_blend\"] = df[pred_col].astype(str).str.strip()\n",
    "\n",
    "# Confiança top-1 — adicionar 'pred_top1_prob_blend' aqui\n",
    "CONF_CANDIDATES = [\n",
    "    \"pred_top1_prob_blend\",     # <-- seu caso\n",
    "    \"conf_top1_blend\",\"conf_top1\",\"conf_blend\",\"confidence\",\n",
    "    \"prob_top1_blend\",\"prob_top1\",\"score_top1\",\"model_conf\",\"conf_model_top1\"\n",
    "]\n",
    "conf_col = next((c for c in CONF_CANDIDATES if c in df.columns), None)\n",
    "\n",
    "if conf_col is None and \"pred_topk_probs_blend\" in df.columns:\n",
    "    # Extrai a maior prob do array/string de probs\n",
    "    def _extract_max_prob(x):\n",
    "        # aceita lista python em string: \"[0.12, 0.08, ...]\"\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                arr = ast.literal_eval(x)\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        else:\n",
    "            arr = x\n",
    "        try:\n",
    "            return float(np.max(arr))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    df[\"conf_top1_blend\"] = df[\"pred_topk_probs_blend\"].apply(_extract_max_prob).fillna(0.0)\n",
    "else:\n",
    "    if conf_col is None:\n",
    "        found = list(df.columns)\n",
    "        raise ValueError(\n",
    "            \"Não encontrei coluna de confiança. Tente salvar uma das seguintes no CSV:\\n\"\n",
    "            \"  - pred_top1_prob_blend  (recomendado)\\n\"\n",
    "            \"  - conf_top1_blend / prob_top1_blend / ...\\n\"\n",
    "            f\"Colunas existentes: {found[:50]}{' ...' if len(found)>50 else ''}\"\n",
    "        )\n",
    "    df[\"conf_top1_blend\"] = pd.to_numeric(df[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# =======================\n",
    "# 2) Threshold simples\n",
    "# =======================\n",
    "decision = np.where(df[\"conf_top1_blend\"] >= TAU_ACCEPT, \"model\", \"abstain\")\n",
    "df[\"decision\"] = decision\n",
    "df[\"final_pred_label\"] = np.where(df[\"decision\"]==\"model\", df[\"pred_top1_blend\"], \"\")\n",
    "\n",
    "# =======================\n",
    "# 3) Métricas (se houver y_true no CSV)\n",
    "# =======================\n",
    "coverage_model = (df[\"decision\"] == \"model\").mean()\n",
    "acc_all = acc_model_only = np.nan\n",
    "\n",
    "if \"destination_departure\" in df.columns:\n",
    "    y_true_raw = df[\"destination_departure\"].astype(str).str.strip()\n",
    "    df[\"y_true_aligned\"] = y_true_raw.apply(align_label)\n",
    "\n",
    "    m_model = (df[\"decision\"] == \"model\")\n",
    "    if m_model.any():\n",
    "        acc_model_only = (df.loc[m_model, \"final_pred_label\"] == df.loc[m_model, \"y_true_aligned\"]).mean()\n",
    "    # como não estamos usando prior aqui, \"aceitos\" == \"model\"\n",
    "    acc_all = acc_model_only\n",
    "\n",
    "    print(f\"[METRICS] Cobertura(model)={coverage_model:.3f}\")\n",
    "    print(f\"[METRICS] Acc(model-only)={acc_model_only if not np.isnan(acc_model_only) else 'NA'}\")\n",
    "else:\n",
    "    print(f\"[INFO] Sem ground truth no FULL. Cobertura(model)={coverage_model:.3f}\")\n",
    "\n",
    "# =======================\n",
    "# 4) Salvar resultado + métricas\n",
    "# =======================\n",
    "suffix = f\"{MODE}_t{TAU_ACCEPT:.2f}\"\n",
    "OUT_CSV = f\"predicoes_full_blend_thresholded_{suffix}.csv\"\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"[OK] CSV final salvo: {OUT_CSV}\")\n",
    "\n",
    "summary = {\n",
    "    \"mode\": MODE,\n",
    "    \"tau_accept\": TAU_ACCEPT,\n",
    "    \"coverage_model\": float(coverage_model),\n",
    "    \"acc_all\": None if np.isnan(acc_all) else float(acc_all),\n",
    "    \"acc_model_only\": None if np.isnan(acc_model_only) else float(acc_model_only),\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"used_pred_col\": pred_col,\n",
    "    \"used_conf_col\": conf_col if conf_col is not None else \"pred_topk_probs_blend[max]\",\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, f\"metrics_{suffix}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] Métricas salvas em: {os.path.join(OUT_DIR, f'metrics_{suffix}.json')}\")\n"
   ],
   "id": "db47ccb0c0007b5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRICS] Cobertura(model)=1.000\n",
      "[METRICS] Acc(model-only)=0.06094947351011632\n",
      "[OK] CSV final salvo: predicoes_full_blend_thresholded_no_prior_t0.35.csv\n",
      "[OK] Métricas salvas em: eval_report\\metrics_no_prior_t0.35.json\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:17:28.160653Z",
     "start_time": "2025-08-19T17:17:20.612243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, ast, json\n",
    "import numpy as np, pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "ARTS_PATH = \"artifacts_destino_model/artifacts.joblib\"\n",
    "FULL_CSV  = \"predicoes_full_blend.csv\"\n",
    "OUT_DIR   = \"eval_report\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "arts = load(ARTS_PATH)\n",
    "le_dest = arts[\"le_dest\"]\n",
    "classes = np.array(arts[\"classes_model\"])\n",
    "model_labels = set(le_dest.inverse_transform(classes))\n",
    "align = lambda s: s if s in model_labels else \"__OUTROS__\"\n",
    "\n",
    "df = pd.read_csv(FULL_CSV)\n",
    "df[\"pred_top1_blend\"] = df[\"pred_top1_blend\"].astype(str).str.strip()\n",
    "\n",
    "# pega confiança (se tiver outro nome, ajuste aqui)\n",
    "if \"pred_top1_prob_blend\" in df.columns:\n",
    "    df[\"conf_top1_blend\"] = pd.to_numeric(df[\"pred_top1_prob_blend\"], errors=\"coerce\").fillna(0.0)\n",
    "elif \"pred_topk_probs_blend\" in df.columns:\n",
    "    df[\"conf_top1_blend\"] = df[\"pred_topk_probs_blend\"].apply(lambda x: float(np.max(ast.literal_eval(x))))\n",
    "else:\n",
    "    raise ValueError(\"Não achei coluna de confiança (ex.: pred_top1_prob_blend).\")\n",
    "\n",
    "have_gt = \"destination_departure\" in df.columns\n",
    "if have_gt:\n",
    "    df[\"y_true_aligned\"] = df[\"destination_departure\"].astype(str).str.strip().apply(align)\n",
    "\n",
    "rows = []\n",
    "for tau in np.arange(0.05, 0.55, 0.05):\n",
    "    accept = df[\"conf_top1_blend\"] >= tau\n",
    "    cov = float(accept.mean())\n",
    "    if have_gt and accept.any():\n",
    "        acc = float((df.loc[accept,\"pred_top1_blend\"] == df.loc[accept,\"y_true_aligned\"]).mean())\n",
    "    else:\n",
    "        acc = np.nan\n",
    "    rows.append({\"tau\":round(float(tau),2), \"coverage\":cov, \"acc_given_tau\":acc})\n",
    "\n",
    "thr = pd.DataFrame(rows)\n",
    "thr.to_csv(os.path.join(OUT_DIR,\"threshold_sweep_full_blend.csv\"), index=False)\n",
    "print(thr)\n"
   ],
   "id": "35d4feb269fdb349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tau  coverage  acc_given_tau\n",
      "0  0.05       1.0       0.060949\n",
      "1  0.10       1.0       0.060949\n",
      "2  0.15       1.0       0.060949\n",
      "3  0.20       1.0       0.060949\n",
      "4  0.25       1.0       0.060949\n",
      "5  0.30       1.0       0.060949\n",
      "6  0.35       1.0       0.060949\n",
      "7  0.40       1.0       0.060949\n",
      "8  0.45       1.0       0.060949\n",
      "9  0.50       1.0       0.060949\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:21:10.095398Z",
     "start_time": "2025-08-19T17:20:52.761970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, ast, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "FULL_CSV  = \"predicoes_full_blend.csv\"\n",
    "ARTS_PATH = \"artifacts_destino_model/artifacts.joblib\"\n",
    "OUT_DIR   = \"eval_report\"\n",
    "TAU_ACCEPT = 0.50  # escolha seu tau (depois de olhar a curva)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Funções auxiliares\n",
    "# -------------------------\n",
    "def find_conf_column(df):\n",
    "    cand = [\n",
    "        \"pred_top1_prob_blend\",   # <- seu caso\n",
    "        \"conf_top1_blend\",\"conf_top1\",\"conf_blend\",\"confidence\",\n",
    "        \"prob_top1_blend\",\"prob_top1\",\"score_top1\",\"model_conf\",\"conf_model_top1\"\n",
    "    ]\n",
    "    for c in cand:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_conf(s: pd.Series) -> pd.Series:\n",
    "    # to numeric (coerce), e tenta normalizar\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    # Heurística: se já está em [0,1], não faz nada\n",
    "    q95 = x.quantile(0.95)\n",
    "    mx  = float(x.max())\n",
    "    if (q95 <= 1.0 and mx <= 1.0):\n",
    "        return x.clip(0,1)\n",
    "\n",
    "    # Se parece estar em % (0–100), normaliza\n",
    "    if mx <= 100.0:\n",
    "        y = (x / 100.0).clip(0,1)\n",
    "        print(f\"[INFO] Confiança parece estar em %. Normalizado: max={mx:.2f} → agora max={y.max():.4f}\")\n",
    "        return y\n",
    "\n",
    "    # Caso extremo: valores >100 (algo errado), normaliza por max para evitar tudo=1\n",
    "    y = (x / (mx if mx != 0 else 1.0)).clip(0,1)\n",
    "    print(f\"[AVISO] Confiança com escala >100. Normalização por max={mx:.2f}.\")\n",
    "    return y\n",
    "\n",
    "def align_label(lbl: str, model_labels: set) -> str:\n",
    "    return lbl if lbl in model_labels else \"__OUTROS__\"\n",
    "\n",
    "# -------------------------\n",
    "# Carregar artefatos (labels válidas)\n",
    "# -------------------------\n",
    "arts = load(ARTS_PATH)\n",
    "le_dest = arts[\"le_dest\"]\n",
    "classes = np.array(arts[\"classes_model\"])\n",
    "model_labels = set(le_dest.inverse_transform(classes))\n",
    "\n",
    "# -------------------------\n",
    "# Ler full e padronizar colunas\n",
    "# -------------------------\n",
    "df = pd.read_csv(FULL_CSV)\n",
    "\n",
    "# Predição top-1\n",
    "if \"pred_top1_blend\" not in df.columns:\n",
    "    raise ValueError(\"Não encontrei 'pred_top1_blend' no FULL.\")\n",
    "\n",
    "df[\"pred_top1_blend\"] = df[\"pred_top1_blend\"].astype(str).str.strip()\n",
    "\n",
    "# Confiança top-1\n",
    "conf_col = find_conf_column(df)\n",
    "if conf_col is None and \"pred_topk_probs_blend\" in df.columns:\n",
    "    # Extrai a maior prob da lista (string) como fallback\n",
    "    def _extract_max_prob(x):\n",
    "        try:\n",
    "            arr = ast.literal_eval(x) if isinstance(x,str) else x\n",
    "            return float(np.max(arr))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    conf_raw = df[\"pred_topk_probs_blend\"].apply(_extract_max_prob)\n",
    "else:\n",
    "    if conf_col is None:\n",
    "        raise ValueError(\"Não achei coluna de confiança (ex.: 'pred_top1_prob_blend').\")\n",
    "    conf_raw = df[conf_col]\n",
    "\n",
    "# Normaliza confiança para [0,1]\n",
    "df[\"conf_top1_blend\"] = normalize_conf(conf_raw)\n",
    "\n",
    "# -------------------------\n",
    "# Sweep de thresholds (se tiver y_true)\n",
    "# -------------------------\n",
    "have_gt = \"destination_departure\" in df.columns\n",
    "rows = []\n",
    "if have_gt:\n",
    "    df[\"y_true_aligned\"] = df[\"destination_departure\"].astype(str).str.strip().apply(lambda z: align_label(z, model_labels))\n",
    "\n",
    "for tau in np.arange(0.05, 0.95+1e-9, 0.05):\n",
    "    accept = df[\"conf_top1_blend\"] >= tau\n",
    "    cov = float(accept.mean())\n",
    "    if have_gt and accept.any():\n",
    "        acc = float((df.loc[accept,\"pred_top1_blend\"] == df.loc[accept,\"y_true_aligned\"]).mean())\n",
    "    else:\n",
    "        acc = np.nan\n",
    "    rows.append({\"tau\": round(float(tau),2), \"coverage\": cov, \"acc_given_tau\": acc})\n",
    "\n",
    "thr = pd.DataFrame(rows)\n",
    "thr_path = os.path.join(OUT_DIR, \"threshold_sweep_full_blend_fixed.csv\")\n",
    "thr.to_csv(thr_path, index=False)\n",
    "print(thr.head(12))\n",
    "print(f\"[OK] {thr_path} salvo\")\n",
    "\n",
    "# -------------------------\n",
    "# Gerar CSV final thresholded\n",
    "# -------------------------\n",
    "df[\"decision\"] = np.where(df[\"conf_top1_blend\"] >= TAU_ACCEPT, \"model\", \"abstain\")\n",
    "df[\"final_pred_label\"] = np.where(df[\"decision\"]==\"model\", df[\"pred_top1_blend\"], \"\")\n",
    "\n",
    "out_csv = f\"predicoes_full_blend_thresholded_no_prior_t{TAU_ACCEPT:.2f}_fixed.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"[OK] CSV final salvo: {out_csv}\")\n",
    "\n",
    "# Métricas simples (se GT existir)\n",
    "if have_gt:\n",
    "    m = df[\"decision\"]==\"model\"\n",
    "    cov = float(m.mean())\n",
    "    acc_model = float((df.loc[m,\"final_pred_label\"] == df.loc[m,\"y_true_aligned\"]).mean()) if m.any() else np.nan\n",
    "    print(f\"[METRICS] Cobertura(model)={cov:.3f} | Acc(model-only)={acc_model if not np.isnan(acc_model) else 'NA'}\")\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, f\"metrics_no_prior_t{TAU_ACCEPT:.2f}_fixed.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"tau\":TAU_ACCEPT,\"coverage_model\":cov,\"acc_model_only\":None if np.isnan(acc_model) else acc_model}, f, indent=2, ensure_ascii=False)\n"
   ],
   "id": "744614ef50b4007a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Confiança parece estar em %. Normalizado: max=100.00 → agora max=1.0000\n",
      "     tau  coverage  acc_given_tau\n",
      "0   0.05  1.000000       0.060949\n",
      "1   0.10  0.999956       0.060951\n",
      "2   0.15  0.894498       0.063629\n",
      "3   0.20  0.722327       0.039489\n",
      "4   0.25  0.501794       0.047425\n",
      "5   0.30  0.259833       0.034518\n",
      "6   0.35  0.192917       0.040234\n",
      "7   0.40  0.143620       0.042872\n",
      "8   0.45  0.116343       0.029695\n",
      "9   0.50  0.090623       0.031710\n",
      "10  0.55  0.076121       0.034839\n",
      "11  0.60  0.065925       0.037440\n",
      "[OK] eval_report\\threshold_sweep_full_blend_fixed.csv salvo\n",
      "[OK] CSV final salvo: predicoes_full_blend_thresholded_no_prior_t0.50_fixed.csv\n",
      "[METRICS] Cobertura(model)=0.091 | Acc(model-only)=0.03170982091935668\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:25:40.613569Z",
     "start_time": "2025-08-19T17:25:24.603321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, ast, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "FULL_CSV  = \"predicoes_full_blend.csv\"  # tem 'pred_top1_blend' e 'pred_top1_prob_blend'\n",
    "ARTS_PATH = \"artifacts_destino_model/artifacts.joblib\"\n",
    "OUT_DIR   = \"eval_report\"\n",
    "TAU_ACCEPT = 0.50  # ajuste depois de ver a curva IN-VOCAB\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def find_conf_column(df):\n",
    "    cand = [\n",
    "        \"pred_top1_prob_blend\",\n",
    "        \"conf_top1_blend\",\"conf_top1\",\"conf_blend\",\"confidence\",\n",
    "        \"prob_top1_blend\",\"prob_top1\",\"score_top1\",\"model_conf\",\"conf_model_top1\"\n",
    "    ]\n",
    "    for c in cand:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_conf(s: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    q95 = x.quantile(0.95)\n",
    "    mx  = float(x.max())\n",
    "    if (q95 <= 1.0 and mx <= 1.0):\n",
    "        return x.clip(0,1)\n",
    "    if mx <= 100.0:\n",
    "        print(f\"[INFO] Confiança parece estar em %. Normalizado: max={mx:.2f} → agora max=1.0000\")\n",
    "        return (x/100.0).clip(0,1)\n",
    "    print(f\"[AVISO] Escala estranha (>100). Normalizando por max={mx:.2f}.\")\n",
    "    return (x/(mx if mx else 1.0)).clip(0,1)\n",
    "\n",
    "# -------------------------\n",
    "# Carregar artefatos (labels válidas do modelo)\n",
    "# -------------------------\n",
    "arts = load(ARTS_PATH)\n",
    "le_dest = arts[\"le_dest\"]\n",
    "classes = np.array(arts[\"classes_model\"])          # índices das classes que o modelo usa\n",
    "model_labels = set(le_dest.inverse_transform(classes))  # nomes (hashes) dessas classes\n",
    "\n",
    "# -------------------------\n",
    "# Ler full e padronizar colunas\n",
    "# -------------------------\n",
    "df = pd.read_csv(FULL_CSV)\n",
    "if \"pred_top1_blend\" not in df.columns:\n",
    "    raise ValueError(\"predicoes_full_blend.csv precisa ter 'pred_top1_blend'.\")\n",
    "\n",
    "df[\"pred_top1_blend\"] = df[\"pred_top1_blend\"].astype(str).str.strip()\n",
    "\n",
    "# Confiança top-1\n",
    "conf_col = find_conf_column(df)\n",
    "if conf_col is None and \"pred_topk_probs_blend\" in df.columns:\n",
    "    def _extract_max_prob(x):\n",
    "        try:\n",
    "            arr = ast.literal_eval(x) if isinstance(x,str) else x\n",
    "            return float(np.max(arr))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    conf_raw = df[\"pred_topk_probs_blend\"].apply(_extract_max_prob)\n",
    "else:\n",
    "    if conf_col is None:\n",
    "        raise ValueError(\"Não achei coluna de confiança (ex.: 'pred_top1_prob_blend').\")\n",
    "    conf_raw = df[conf_col]\n",
    "\n",
    "df[\"conf_top1_blend\"] = normalize_conf(conf_raw)\n",
    "\n",
    "# -------------------------\n",
    "# Construir máscara IN-VOCAB (onde existe y_true nas classes do modelo)\n",
    "# -------------------------\n",
    "have_gt = \"destination_departure\" in df.columns\n",
    "if not have_gt:\n",
    "    print(\"[AVISO] FULL sem ground-truth; não dá pra medir acurácia.\")\n",
    "    # mesmo assim gera thresholded por confiança\n",
    "    mask_accept = df[\"conf_top1_blend\"] >= TAU_ACCEPT\n",
    "    df[\"decision\"] = np.where(mask_accept, \"model\", \"abstain\")\n",
    "    df[\"final_pred_label\"] = np.where(mask_accept, df[\"pred_top1_blend\"], \"\")\n",
    "    out_csv = f\"predicoes_full_blend_thresholded_invoc_t{TAU_ACCEPT:.2f}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] {out_csv} salvo (sem métricas).\")\n",
    "else:\n",
    "    df[\"destination_departure\"] = df[\"destination_departure\"].astype(str).str.strip()\n",
    "    in_vocab = df[\"destination_departure\"].isin(model_labels)\n",
    "    out_vocab = ~in_vocab\n",
    "    print(f\"[SPLIT] IN-VOCAB={in_vocab.mean():.3f} | OUT-OF-VOCAB={out_vocab.mean():.3f}\")\n",
    "\n",
    "    # -------- Sweep só em IN-VOCAB --------\n",
    "    rows = []\n",
    "    for tau in np.arange(0.05, 0.95+1e-9, 0.05):\n",
    "        accept = (df[\"conf_top1_blend\"] >= tau) & in_vocab\n",
    "        cov = float(accept.mean())  # cobertura SOBRE TODO FULL (pra ver impacto global)\n",
    "        # acurácia condicional SOMENTE nas linhas aceitas IN-VOCAB\n",
    "        if accept.any():\n",
    "            acc = float((df.loc[accept,\"pred_top1_blend\"] == df.loc[accept,\"destination_departure\"]).mean())\n",
    "        else:\n",
    "            acc = np.nan\n",
    "        rows.append({\"tau\": round(float(tau),2), \"coverage_overall\": cov, \"acc_on_accepted_in_vocab\": acc})\n",
    "    thr_in = pd.DataFrame(rows)\n",
    "    thr_in_path = os.path.join(OUT_DIR, \"threshold_sweep_in_vocab.csv\")\n",
    "    thr_in.to_csv(thr_in_path, index=False)\n",
    "    print(f\"[OK] {thr_in_path} salvo\")\n",
    "    print(thr_in.head(12))\n",
    "\n",
    "    # -------- Relato do OUT-OF-VOCAB (taxa de cauda) --------\n",
    "    tail_rate = float(out_vocab.mean())\n",
    "    with open(os.path.join(OUT_DIR,\"tail_rate.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump({\"tail_rate\": tail_rate, \"in_vocab_rate\": 1.0-tail_rate}, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"[TAIL] OUT-OF-VOCAB rate ~ {tail_rate:.3f} salvo em eval_report/tail_rate.json\")\n",
    "\n",
    "    # -------- CSV final thresholded (aceita só IN-VOCAB confiantes) --------\n",
    "    accept_final = (df[\"conf_top1_blend\"] >= TAU_ACCEPT) & in_vocab\n",
    "    df[\"decision\"] = np.where(accept_final, \"model\", \"abstain\")\n",
    "    df[\"final_pred_label\"] = np.where(accept_final, df[\"pred_top1_blend\"], \"\")\n",
    "    out_csv = f\"predicoes_full_blend_thresholded_invoc_t{TAU_ACCEPT:.2f}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] CSV final salvo: {out_csv}\")\n",
    "\n",
    "    # Métricas resumidas\n",
    "    cov_overall = float(accept_final.mean())\n",
    "    if accept_final.any():\n",
    "        acc_accept = float((df.loc[accept_final,\"final_pred_label\"] == df.loc[accept_final,\"destination_departure\"]).mean())\n",
    "    else:\n",
    "        acc_accept = np.nan\n",
    "    with open(os.path.join(OUT_DIR, f\"metrics_in_vocab_t{TAU_ACCEPT:.2f}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"tau\": TAU_ACCEPT,\n",
    "            \"coverage_overall\": cov_overall,\n",
    "            \"acc_on_accepted_in_vocab\": None if np.isnan(acc_accept) else acc_accept,\n",
    "            \"tail_rate\": tail_rate\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"[METRICS] τ={TAU_ACCEPT:.2f} | cobertura_total={cov_overall:.3f} | acc(aceitos IN-VOCAB)={acc_accept if not np.isnan(acc_accept) else 'NA'}\")\n"
   ],
   "id": "be091e4bd6a3dbc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Confiança parece estar em %. Normalizado: max=100.00 → agora max=1.0000\n",
      "[SPLIT] IN-VOCAB=0.000 | OUT-OF-VOCAB=1.000\n",
      "[OK] eval_report\\threshold_sweep_in_vocab.csv salvo\n",
      "     tau  coverage_overall  acc_on_accepted_in_vocab\n",
      "0   0.05               0.0                       NaN\n",
      "1   0.10               0.0                       NaN\n",
      "2   0.15               0.0                       NaN\n",
      "3   0.20               0.0                       NaN\n",
      "4   0.25               0.0                       NaN\n",
      "5   0.30               0.0                       NaN\n",
      "6   0.35               0.0                       NaN\n",
      "7   0.40               0.0                       NaN\n",
      "8   0.45               0.0                       NaN\n",
      "9   0.50               0.0                       NaN\n",
      "10  0.55               0.0                       NaN\n",
      "11  0.60               0.0                       NaN\n",
      "[TAIL] OUT-OF-VOCAB rate ~ 1.000 salvo em eval_report/tail_rate.json\n",
      "[OK] CSV final salvo: predicoes_full_blend_thresholded_invoc_t0.50.csv\n",
      "[METRICS] τ=0.50 | cobertura_total=0.000 | acc(aceitos IN-VOCAB)=NA\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:32:28.282789Z",
     "start_time": "2025-08-19T17:32:10.768018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, ast, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "FULL_CSV  = \"predicoes_full_blend.csv\"   # precisa ter pred_top1_blend e a coluna de confiança (ex: pred_top1_prob_blend)\n",
    "ARTS_PATH = \"artifacts_destino_model/artifacts.joblib\"\n",
    "OUT_DIR   = \"eval_report\"\n",
    "TAU_ACCEPT = 0.50\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def find_conf_column(df):\n",
    "    cand = [\n",
    "        \"pred_top1_prob_blend\",\n",
    "        \"conf_top1_blend\",\"conf_top1\",\"conf_blend\",\"confidence\",\n",
    "        \"prob_top1_blend\",\"prob_top1\",\"score_top1\",\"model_conf\",\"conf_model_top1\"\n",
    "    ]\n",
    "    for c in cand:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_conf(s: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    q95 = x.quantile(0.95)\n",
    "    mx  = float(x.max())\n",
    "    if (q95 <= 1.0 and mx <= 1.0):\n",
    "        return x.clip(0,1)\n",
    "    if mx <= 100.0:\n",
    "        print(f\"[INFO] Confiança parece estar em %. Normalizado: max={mx:.2f} → agora max=1.0000\")\n",
    "        return (x/100.0).clip(0,1)\n",
    "    print(f\"[AVISO] Escala estranha (>100). Normalizando por max={mx:.2f}.\")\n",
    "    return (x/(mx if mx else 1.0)).clip(0,1)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Artefatos e reconstrução robusta do vocabulário\n",
    "# -------------------------\n",
    "arts = load(ARTS_PATH)\n",
    "\n",
    "def _to_set_str_lower(x):\n",
    "    return set(map(lambda z: str(z).strip().lower(), x))\n",
    "\n",
    "model_labels = set()\n",
    "\n",
    "# a) nomes mantidos explicitamente (se existirem)\n",
    "if \"kept_label_names\" in arts:\n",
    "    model_labels |= _to_set_str_lower(arts[\"kept_label_names\"])\n",
    "\n",
    "# b) via le_dest + classes_model (índices ou nomes)\n",
    "if \"le_dest\" in arts:\n",
    "    le_dest = arts[\"le_dest\"]\n",
    "    if \"classes_model\" in arts:\n",
    "        cm = np.array(arts[\"classes_model\"])\n",
    "        try:\n",
    "            # se for numérico, inverta pelo encoder\n",
    "            if np.issubdtype(cm.dtype, np.integer) or np.issubdtype(cm.dtype, np.floating):\n",
    "                cm_int = cm.astype(int)\n",
    "                inv = le_dest.inverse_transform(cm_int)\n",
    "                model_labels |= _to_set_str_lower(inv)\n",
    "            else:\n",
    "                model_labels |= _to_set_str_lower(cm)\n",
    "        except Exception as e:\n",
    "            print(\"[AVISO] Falha ao inverse_transform classes_model; vou tentar interpretar como nomes. Erro:\", e)\n",
    "            model_labels |= _to_set_str_lower(cm)\n",
    "\n",
    "# c) adiciona chaves dos priors (normalmente refletem o vocabulário treinado)\n",
    "if \"prior_map\" in arts and isinstance(arts[\"prior_map\"], dict):\n",
    "    for src, dest_counts in arts[\"prior_map\"].items():\n",
    "        try:\n",
    "            model_labels |= _to_set_str_lower(dest_counts.keys())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if \"global_prior\" in arts and isinstance(arts[\"global_prior\"], dict):\n",
    "    model_labels |= _to_set_str_lower(arts[\"global_prior\"].keys())\n",
    "\n",
    "print(f\"[VOCAB] Rótulos únicos recuperados: {len(model_labels)}\")\n",
    "\n",
    "# Sanidade: se ainda estiver vazio, aborta com instrução clara\n",
    "if len(model_labels) == 0:\n",
    "    raise RuntimeError(\"Vocabulário do modelo ficou vazio. Confira o artifacts.joblib: \"\n",
    "                       \"precisa ter kept_label_names, ou classes_model + le_dest, ou prior_map/global_prior.\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Ler FULL e normalizar colunas\n",
    "# -------------------------\n",
    "df = pd.read_csv(FULL_CSV)\n",
    "if \"pred_top1_blend\" not in df.columns:\n",
    "    raise ValueError(\"O CSV full precisa ter 'pred_top1_blend'.\")\n",
    "\n",
    "df[\"pred_top1_blend\"] = df[\"pred_top1_blend\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# confiança top-1\n",
    "conf_col = find_conf_column(df)\n",
    "if conf_col is None and \"pred_topk_probs_blend\" in df.columns:\n",
    "    def _extract_max_prob(x):\n",
    "        try:\n",
    "            arr = ast.literal_eval(x) if isinstance(x,str) else x\n",
    "            return float(np.max(arr))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    conf_raw = df[\"pred_topk_probs_blend\"].apply(_extract_max_prob)\n",
    "else:\n",
    "    if conf_col is None:\n",
    "        raise ValueError(\"Não achei coluna de confiança (ex.: 'pred_top1_prob_blend').\")\n",
    "    conf_raw = df[conf_col]\n",
    "\n",
    "df[\"conf_top1_blend\"] = normalize_conf(conf_raw)\n",
    "\n",
    "# ground-truth normalizado (se existir)\n",
    "have_gt = \"destination_departure\" in df.columns\n",
    "if not have_gt:\n",
    "    print(\"[AVISO] FULL sem ground-truth; não dá pra medir acurácia.\")\n",
    "    mask_accept = (df[\"conf_top1_blend\"] >= TAU_ACCEPT)\n",
    "    df[\"decision\"] = np.where(mask_accept, \"model\", \"abstain\")\n",
    "    df[\"final_pred_label\"] = np.where(mask_accept, df[\"pred_top1_blend\"], \"\")\n",
    "    out_csv = f\"predicoes_full_blend_thresholded_invoc_t{TAU_ACCEPT:.2f}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] {out_csv} salvo (sem métricas).\")\n",
    "else:\n",
    "    df[\"destination_departure\"] = df[\"destination_departure\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Split IN-VOCAB vs OUT-OF-VOCAB (robusto)\n",
    "    # -------------------------\n",
    "    in_vocab = df[\"destination_departure\"].isin(model_labels)\n",
    "    out_vocab = ~in_vocab\n",
    "\n",
    "    # Diagnóstico se der baixo demais\n",
    "    rate_in = float(in_vocab.mean())\n",
    "    print(f\"[SPLIT] IN-VOCAB={rate_in:.3f} | OUT-OF-VOCAB={float(out_vocab.mean()):.3f}\")\n",
    "\n",
    "    if rate_in == 0.0:\n",
    "        # Mostra pistas para depurar\n",
    "        gt_sample = set(df[\"destination_departure\"].head(1000).unique())\n",
    "        inter = gt_sample & model_labels\n",
    "        print(\"[DEBUG] Nenhum match nos 1000 primeiros GT; exemplos de GT:\", list(sorted(list(gt_sample))[:5]))\n",
    "        print(\"[DEBUG] Exemplos do VOCAB:\", list(sorted(list(model_labels))[:5]))\n",
    "        print(\"→ Verifique se o FULL e o treino foram normalizados do mesmo jeito (trim/lower/hash).\")\n",
    "        # ainda assim, gera o CSV thresholded (tudo abstain)\n",
    "        accept_final = (df[\"conf_top1_blend\"] >= TAU_ACCEPT) & in_vocab\n",
    "        df[\"decision\"] = np.where(accept_final, \"model\", \"abstain\")\n",
    "        df[\"final_pred_label\"] = np.where(accept_final, df[\"pred_top1_blend\"], \"\")\n",
    "        out_csv = f\"predicoes_full_blend_thresholded_invoc_t{TAU_ACCEPT:.2f}.csv\"\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"[OK] CSV final salvo (tudo abstain): {out_csv}\")\n",
    "    else:\n",
    "        # -------------------------\n",
    "        # 4) Sweep só em IN-VOCAB\n",
    "        # -------------------------\n",
    "        rows = []\n",
    "        for tau in np.arange(0.05, 0.95 + 1e-9, 0.05):\n",
    "            accept = (df[\"conf_top1_blend\"] >= tau) & in_vocab\n",
    "            cov = float(accept.mean())  # cobertura sobre o FULL (impacto global)\n",
    "            if accept.any():\n",
    "                acc = float((df.loc[accept, \"pred_top1_blend\"] ==\n",
    "                             df.loc[accept, \"destination_departure\"]).mean())\n",
    "            else:\n",
    "                acc = np.nan\n",
    "            rows.append({\n",
    "                \"tau\": round(float(tau), 2),\n",
    "                \"coverage_overall\": cov,\n",
    "                \"acc_on_accepted_in_vocab\": acc\n",
    "            })\n",
    "        thr_in = pd.DataFrame(rows)\n",
    "        thr_in_path = os.path.join(OUT_DIR, \"threshold_sweep_in_vocab.csv\")\n",
    "        thr_in.to_csv(thr_in_path, index=False)\n",
    "        print(f\"[OK] {thr_in_path} salvo\")\n",
    "        print(thr_in.head(12))\n",
    "\n",
    "        # -------------------------\n",
    "        # 5) CSV final thresholded (aceita só IN-VOCAB confiantes)\n",
    "        # -------------------------\n",
    "        accept_final = (df[\"conf_top1_blend\"] >= TAU_ACCEPT) & in_vocab\n",
    "        df[\"decision\"] = np.where(accept_final, \"model\", \"abstain\")\n",
    "        df[\"final_pred_label\"] = np.where(accept_final, df[\"pred_top1_blend\"], \"\")\n",
    "        out_csv = f\"predicoes_full_blend_thresholded_invoc_t{TAU_ACCEPT:.2f}.csv\"\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"[OK] CSV final salvo: {out_csv}\")\n",
    "\n",
    "        # métricas resumidas\n",
    "        cov_overall = float(accept_final.mean())\n",
    "        if accept_final.any():\n",
    "            acc_accept = float((df.loc[accept_final, \"final_pred_label\"] ==\n",
    "                                df.loc[accept_final, \"destination_departure\"]).mean())\n",
    "        else:\n",
    "            acc_accept = np.nan\n",
    "        with open(os.path.join(OUT_DIR, f\"metrics_in_vocab_t{TAU_ACCEPT:.2f}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"tau\": TAU_ACCEPT,\n",
    "                \"coverage_overall\": cov_overall,\n",
    "                \"acc_on_accepted_in_vocab\": None if np.isnan(acc_accept) else acc_accept,\n",
    "                \"tail_rate\": float(out_vocab.mean())\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[METRICS] τ={TAU_ACCEPT:.2f} | cobertura_total={cov_overall:.3f} | acc(aceitos IN-VOCAB)={acc_accept if not np.isnan(acc_accept) else 'NA'}\")\n"
   ],
   "id": "fd35d651e6625a69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VOCAB] Rótulos únicos recuperados: 333\n",
      "[INFO] Confiança parece estar em %. Normalizado: max=100.00 → agora max=1.0000\n",
      "[SPLIT] IN-VOCAB=0.000 | OUT-OF-VOCAB=1.000\n",
      "[DEBUG] Nenhum match nos 1000 primeiros GT; exemplos de GT: ['1', '100', '102', '103', '104']\n",
      "[DEBUG] Exemplos do VOCAB: ['011af72a910ac4acf367eef9e6b761e0980842c30d4e9809840f4141d5163ede', '01a0123885ebec5b37b52ddc058c20d052525c654b69e7b7bfd5feb291428bba', '0228374d12ee995cdeff8e25d819990f80d40c8773321fafe4ce1572c7df29af', '02d20bbd7e394ad5999a4cebabac9619732c343a4cac99470c03e23ba2bdc2bc', '0359bdb72fa3b3ee4adfdfcae848c4b5cc3c889bc7400efc01051e9cd67482c9']\n",
      "→ Verifique se o FULL e o treino foram normalizados do mesmo jeito (trim/lower/hash).\n",
      "[OK] CSV final salvo (tudo abstain): predicoes_full_blend_thresholded_invoc_t0.50.csv\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:34:22.745620Z",
     "start_time": "2025-08-19T17:34:19.854856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- depois de:\n",
    "# arts = load(ARTS_PATH)\n",
    "# df = pd.read_csv(FULL_CSV)\n",
    "# e antes de usar destination_departure para split/métricas ---\n",
    "\n",
    "# 1) Garante que temos o encoder\n",
    "if \"le_dest\" not in arts:\n",
    "    raise RuntimeError(\"Artefatos não têm 'le_dest'. Não consigo decodificar rótulos numéricos do ground-truth.\")\n",
    "\n",
    "le_dest = arts[\"le_dest\"]\n",
    "\n",
    "# 2) Detecta e decodifica ground-truth numérico\n",
    "def _looks_numeric_series(s: pd.Series) -> bool:\n",
    "    # True se todos os valores forem inteiros válidos (inclusive strings de dígitos)\n",
    "    try:\n",
    "        tmp = pd.to_numeric(s, errors=\"coerce\")\n",
    "        return tmp.notna().all() and (tmp.astype(int) == tmp).all()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if \"destination_departure\" in df.columns:\n",
    "    gt = df[\"destination_departure\"]\n",
    "\n",
    "    if _looks_numeric_series(gt):\n",
    "        # Converte para int e aplica inverse_transform\n",
    "        gt_int = pd.to_numeric(gt, errors=\"coerce\").astype(\"Int64\")\n",
    "        if gt_int.isna().any():\n",
    "            raise ValueError(\"Há valores não numéricos em 'destination_departure' que não consigo converter.\")\n",
    "\n",
    "        # inverse_transform espera array de int sem NA\n",
    "        gt_decoded = le_dest.inverse_transform(gt_int.astype(int).to_numpy())\n",
    "        df[\"destination_departure\"] = pd.Series(gt_decoded, index=df.index)\n",
    "        print(\"[FIX] Ground-truth decodificado via le_dest.inverse_transform().\")\n",
    "\n",
    "    # Normalização final (igual ao treino)\n",
    "    df[\"destination_departure\"] = df[\"destination_departure\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# 3) Também normalize as colunas de predição/confiança (mantendo o que você já fazia)\n",
    "df[\"pred_top1_blend\"] = df[\"pred_top1_blend\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# reusa sua função find_conf_column/normalize_conf para montar df[\"conf_top1_blend\"]...\n"
   ],
   "id": "3707581a628c380e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIX] Ground-truth decodificado via le_dest.inverse_transform().\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:37:49.490848Z",
     "start_time": "2025-08-19T17:37:15.587061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, json, math, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====== ENTRADAS ======\n",
    "ARTS_PATH   = \"artifacts_destino_model/artifacts.joblib\"\n",
    "FULL_CSV    = \"predicoes_full_blend.csv\"   # já carregado no seu fluxo\n",
    "OUT_DIR     = \"eval_report\"\n",
    "TAU_DEFAULT = 0.35                         # ajuste se desejar\n",
    "\n",
    "# Se ainda não tiver em memória:\n",
    "# from joblib import load\n",
    "# arts = load(ARTS_PATH)\n",
    "# df   = pd.read_csv(FULL_CSV)\n",
    "\n",
    "# ---------- 1) Normalizações e vocabulário ----------\n",
    "# GT já foi decodificado via le_dest.inverse_transform() no seu passo anterior.\n",
    "# Ainda assim, garantimos normalização idêntica:\n",
    "df[\"destination_departure\"] = df[\"destination_departure\"].astype(str).str.strip().str.lower()\n",
    "df[\"pred_top1_blend\"]       = df[\"pred_top1_blend\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Vocabulário do modelo\n",
    "model_labels = set([str(x).strip().lower() for x in arts.get(\"classes_model\", [])])\n",
    "if not model_labels:\n",
    "    # fallback: do encoder\n",
    "    le_dest = arts[\"le_dest\"]\n",
    "    model_labels = set([str(x).strip().lower() for x in le_dest.classes_])\n",
    "\n",
    "# ---------- 2) Confiança (detecta a coluna automaticamente) ----------\n",
    "CONF_CANDIDATES = [\n",
    "    \"conf_top1_blend\", \"pred_top1_prob_blend\", \"conf_top1\", \"confidence\",\n",
    "    \"prob_top1_blend\", \"prob_top1\", \"score_top1\", \"model_conf\"\n",
    "]\n",
    "conf_col = None\n",
    "for c in CONF_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        conf_col = c\n",
    "        break\n",
    "if conf_col is None:\n",
    "    raise ValueError(f\"Não encontrei coluna de confiança. Tentei: {CONF_CANDIDATES}. \"\n",
    "                     f\"Colunas no CSV: {list(df.columns)[:40]}{' ...' if df.shape[1]>40 else ''}\")\n",
    "\n",
    "conf = pd.to_numeric(df[conf_col], errors=\"coerce\")\n",
    "# Normaliza se vier em %\n",
    "if conf.max() > 1.5:\n",
    "    print(f\"[INFO] Confiança em % detectada (max={conf.max():.2f}). Convertendo para [0,1].\")\n",
    "    conf = conf / 100.0\n",
    "df[\"conf_top1_blend\"] = conf.clip(0, 1)\n",
    "\n",
    "# ---------- 3) Top-k (se disponível) ----------\n",
    "def _safe_parse_list(x):\n",
    "    # aceita string \"[...]\" ou já lista/array\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return list(x)\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "has_topk = (\"pred_topk_labels_blend\" in df.columns) and (\"pred_topk_probs_blend\" in df.columns)\n",
    "if has_topk:\n",
    "    topk_labels = df[\"pred_topk_labels_blend\"].apply(_safe_parse_list).apply(\n",
    "        lambda arr: [str(z).strip().lower() for z in arr]\n",
    "    )\n",
    "    topk_probs  = df[\"pred_topk_probs_blend\"].apply(_safe_parse_list)\n",
    "else:\n",
    "    topk_labels = None\n",
    "    topk_probs  = None\n",
    "\n",
    "# ---------- 4) Split IN-VOCAB ----------\n",
    "in_vocab_mask = df[\"destination_departure\"].isin(model_labels)\n",
    "inv_rate = 1.0 - in_vocab_mask.mean()\n",
    "print(f\"[SPLIT] IN-VOCAB={in_vocab_mask.mean():.3f} | OUT-OF-VOCAB={inv_rate:.3f}\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# Pequeno dump do tail rate\n",
    "with open(os.path.join(OUT_DIR, \"tail_rate.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"out_of_vocab_rate\": inv_rate}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---------- 5) Métricas Top-1 / Top-3 ----------\n",
    "def top1_acc(df_, mask=None):\n",
    "    m = mask if mask is not None else np.ones(len(df_), dtype=bool)\n",
    "    if m.sum() == 0: return float(\"nan\")\n",
    "    return (df_.loc[m, \"pred_top1_blend\"] == df_.loc[m, \"destination_departure\"]).mean()\n",
    "\n",
    "def top3_acc(df_, mask=None):\n",
    "    if not has_topk: return float(\"nan\")\n",
    "    m = mask if mask is not None else np.ones(len(df_), dtype=bool)\n",
    "    if m.sum() == 0: return float(\"nan\")\n",
    "    ok = []\n",
    "    gt = df_.loc[m, \"destination_departure\"].values\n",
    "    for i, row_labels in enumerate(topk_labels[m]):\n",
    "        ok.append(gt[i] in set(row_labels[:3]))\n",
    "    return float(np.mean(ok)) if ok else float(\"nan\")\n",
    "\n",
    "acc_overall  = top1_acc(df)\n",
    "top3_overall = top3_acc(df)\n",
    "print(f\"[RESULT] Top-1(overall)= {acc_overall:.4f} | Top-3(overall)= {top3_overall:.4f}\")\n",
    "\n",
    "acc_in  = top1_acc(df, in_vocab_mask)\n",
    "top3_in = top3_acc(df, in_vocab_mask)\n",
    "print(f\"[RESULT IN-VOCAB] Top-1= {acc_in:.4f} | Top-3= {top3_in:.4f}\")\n",
    "\n",
    "# ---------- 6) Calibração (ECE em 10 bins, Top-1) ----------\n",
    "def ece_top1(conf, correct, n_bins=10):\n",
    "    bins = np.linspace(0, 1, n_bins+1)\n",
    "    idx  = np.digitize(conf, bins) - 1\n",
    "    ece = 0.0; rows=[]\n",
    "    for b in range(n_bins):\n",
    "        m = idx == b\n",
    "        if m.sum() == 0:\n",
    "            rows.append((f\"({bins[b]:.1f},{bins[b+1]:.1f}]\", 0, np.nan, np.nan, np.nan))\n",
    "            continue\n",
    "        conf_m = conf[m].mean()\n",
    "        acc_m  = correct[m].mean()\n",
    "        rows.append((f\"({bins[b]:.1f},{bins[b+1]:.1f}]\", int(m.sum()), conf_m, acc_m, acc_m - conf_m))\n",
    "        ece += (m.mean()) * abs(acc_m - conf_m)\n",
    "    return ece, pd.DataFrame(rows, columns=[\"bin\",\"n\",\"conf_m\",\"acc_m\",\"gap\"])\n",
    "\n",
    "correct_overall = (df[\"pred_top1_blend\"] == df[\"destination_departure\"]).astype(float).values\n",
    "ece, calib_df = ece_top1(df[\"conf_top1_blend\"].values, correct_overall, n_bins=10)\n",
    "calib_df.to_csv(os.path.join(OUT_DIR, \"calibration_bins.csv\"), index=False)\n",
    "print(f\"[CALIB] ECE(Top-1) = {ece:.4f} | bins salvos em {OUT_DIR}/calibration_bins.csv\")\n",
    "\n",
    "# ---------- 7) Threshold sweep (τ) ----------\n",
    "def sweep_threshold(df_, taus=np.arange(0.05, 0.61, 0.05)):\n",
    "    rows = []\n",
    "    for t in taus:\n",
    "        acc_m = float(\"nan\")\n",
    "        m = df_[\"conf_top1_blend\"] >= t\n",
    "        cov = m.mean()\n",
    "        if cov > 0:\n",
    "            acc_m = (df_.loc[m, \"pred_top1_blend\"] == df_.loc[m, \"destination_departure\"]).mean()\n",
    "        rows.append({\"tau\": float(t), \"coverage\": float(cov), \"acc_given_tau\": float(acc_m if not math.isnan(acc_m) else np.nan)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "thr_df = sweep_threshold(df)\n",
    "thr_df.to_csv(os.path.join(OUT_DIR, \"threshold_sweep_full_blend.csv\"), index=False)\n",
    "print(f\"[OK] {OUT_DIR}/threshold_sweep_full_blend.csv salvo\")\n",
    "\n",
    "# ---------- 8) CSV final thresholded ----------\n",
    "tau = TAU_DEFAULT\n",
    "accepted = df[\"conf_top1_blend\"] >= tau\n",
    "out = df.copy()\n",
    "out[\"accepted\"] = accepted.astype(int)     # 1 = modelo confiante, 0 = abstain\n",
    "# (opcional) marque também se o GT estava no vocabulário do modelo\n",
    "out[\"in_vocab\"] = in_vocab_mask.astype(int)\n",
    "\n",
    "FINAL_CSV = f\"predicoes_full_blend_thresholded_no_prior_t{tau:.2f}.csv\".replace(\",\", \".\")\n",
    "out.to_csv(FINAL_CSV, index=False)\n",
    "print(f\"[OK] CSV final salvo: {FINAL_CSV}\")\n",
    "\n",
    "# ---------- 9) Métricas → JSON ----------\n",
    "metrics = {\n",
    "    \"top1_overall\": float(acc_overall),\n",
    "    \"top3_overall\": float(top3_overall) if not math.isnan(top3_overall) else None,\n",
    "    \"top1_in_vocab\": float(acc_in) if not math.isnan(acc_in) else None,\n",
    "    \"top3_in_vocab\": float(top3_in) if not math.isnan(top3_in) else None,\n",
    "    \"ece_top1\": float(ece),\n",
    "    \"threshold\": float(tau),\n",
    "    \"coverage_at_tau\": float(accepted.mean()),\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"metrics_full_blend.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] Métricas salvas em: {OUT_DIR}/metrics_full_blend.json\")\n",
    "\n"
   ],
   "id": "3b7bdedd15bb7dc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPLIT] IN-VOCAB=0.000 | OUT-OF-VOCAB=1.000\n",
      "[RESULT] Top-1(overall)= 0.3253 | Top-3(overall)= 0.5633\n",
      "[RESULT IN-VOCAB] Top-1= nan | Top-3= nan\n",
      "[CALIB] ECE(Top-1) = 0.0459 | bins salvos em eval_report/calibration_bins.csv\n",
      "[OK] eval_report/threshold_sweep_full_blend.csv salvo\n",
      "[OK] CSV final salvo: predicoes_full_blend_thresholded_no_prior_t0.35.csv\n",
      "[OK] Métricas salvas em: eval_report/metrics_full_blend.json\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:42:30.926277Z",
     "start_time": "2025-08-19T17:42:03.848860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, json, math, ast, numpy as np, pandas as pd\n",
    "\n",
    "OUT_DIR = \"eval_report\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Normalização mínima (mantenha seu df carregado) ---\n",
    "df[\"destination_departure\"] = df[\"destination_departure\"].astype(str).str.strip().str.lower()\n",
    "df[\"pred_top1_blend\"]       = df[\"pred_top1_blend\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# --- VOCAB corrigido ---\n",
    "le_dest = arts[\"le_dest\"]\n",
    "def _build_model_labels(arts):\n",
    "    cls = np.array(list(arts.get(\"classes_model\", [])))\n",
    "    base = None\n",
    "    if cls.size > 0:\n",
    "        if np.issubdtype(cls.dtype, np.integer):\n",
    "            try:\n",
    "                base = le_dest.inverse_transform(cls)\n",
    "            except Exception:\n",
    "                base = None\n",
    "        if base is None and cls.dtype.kind in (\"U\", \"S\", \"O\"):\n",
    "            base = cls\n",
    "    if base is None:\n",
    "        base = le_dest.classes_\n",
    "    return set(str(x).strip().lower() for x in base)\n",
    "\n",
    "model_labels = _build_model_labels(arts)\n",
    "\n",
    "# --- Confiança (detecção automática já usada antes) ---\n",
    "conf_col = \"conf_top1_blend\" if \"conf_top1_blend\" in df.columns else \"pred_top1_prob_blend\"\n",
    "conf = pd.to_numeric(df[conf_col], errors=\"coerce\")\n",
    "if conf.max() > 1.5:\n",
    "    print(f\"[INFO] Confiança em % detectada (max={conf.max():.2f}). Convertendo para [0,1].\")\n",
    "    conf = conf / 100.0\n",
    "df[\"conf_top1_blend\"] = conf.clip(0,1)\n",
    "\n",
    "# --- Top-k (se existir) ---\n",
    "def _safe_parse_list(x):\n",
    "    if isinstance(x, (list, tuple, np.ndarray)): return list(x)\n",
    "    if isinstance(x, str):\n",
    "        try: return ast.literal_eval(x)\n",
    "        except: return []\n",
    "    return []\n",
    "has_topk = (\"pred_topk_labels_blend\" in df.columns) and (\"pred_topk_probs_blend\" in df.columns)\n",
    "if has_topk:\n",
    "    topk_labels = df[\"pred_topk_labels_blend\"].apply(_safe_parse_list).apply(lambda arr: [str(z).strip().lower() for z in arr])\n",
    "else:\n",
    "    topk_labels = None\n",
    "\n",
    "# --- Split IN/OUT agora correto ---\n",
    "in_vocab_mask = df[\"destination_departure\"].isin(model_labels)\n",
    "print(f\"[SPLIT] IN-VOCAB={in_vocab_mask.mean():.3f} | OUT-OF-VOCAB={1 - in_vocab_mask.mean():.3f}\")\n",
    "with open(os.path.join(OUT_DIR, \"tail_rate.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"out_of_vocab_rate\": float(1 - in_vocab_mask.mean())}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- Métricas ---\n",
    "def top1_acc(mask=None):\n",
    "    m = np.ones(len(df), dtype=bool) if mask is None else mask\n",
    "    if m.sum()==0: return float(\"nan\")\n",
    "    return (df.loc[m,\"pred_top1_blend\"]==df.loc[m,\"destination_departure\"]).mean()\n",
    "\n",
    "def top3_acc(mask=None):\n",
    "    if not has_topk: return float(\"nan\")\n",
    "    m = np.ones(len(df), dtype=bool) if mask is None else mask\n",
    "    if m.sum()==0: return float(\"nan\")\n",
    "    ok=[]; gt=df.loc[m,\"destination_departure\"].values\n",
    "    tkl = topk_labels[m]\n",
    "    for i, labs in enumerate(tkl):\n",
    "        ok.append(gt[i] in set(labs[:3]))\n",
    "    return float(np.mean(ok)) if ok else float(\"nan\")\n",
    "\n",
    "acc_overall, top3_overall = top1_acc(), top3_acc()\n",
    "acc_in, top3_in          = top1_acc(in_vocab_mask), top3_acc(in_vocab_mask)\n",
    "print(f\"[RESULT] Top-1(overall)= {acc_overall:.4f} | Top-3(overall)= {top3_overall:.4f}\")\n",
    "print(f\"[RESULT IN-VOCAB] Top-1= {acc_in:.4f} | Top-3= {top3_in:.4f}\")\n",
    "\n",
    "# --- ECE simples ---\n",
    "def ece_top1(conf, correct, n_bins=10):\n",
    "    bins=np.linspace(0,1,n_bins+1); idx=np.digitize(conf,bins)-1; ece=0; rows=[]\n",
    "    for b in range(n_bins):\n",
    "        m = idx==b\n",
    "        if m.sum()==0:\n",
    "            rows.append((f\"({bins[b]:.1f},{bins[b+1]:.1f}]\",0,np.nan,np.nan,np.nan)); continue\n",
    "        cm=conf[m].mean(); am=correct[m].mean()\n",
    "        rows.append((f\"({bins[b]:.1f},{bins[b+1]:.1f}]\",int(m.sum()),cm,am,am-cm))\n",
    "        ece += (m.mean())*abs(am-cm)\n",
    "    return ece, pd.DataFrame(rows, columns=[\"bin\",\"n\",\"conf_m\",\"acc_m\",\"gap\"])\n",
    "\n",
    "correct = (df[\"pred_top1_blend\"]==df[\"destination_departure\"]).astype(float).values\n",
    "ece, calib_df = ece_top1(df[\"conf_top1_blend\"].values, correct, n_bins=10)\n",
    "calib_df.to_csv(os.path.join(OUT_DIR,\"calibration_bins.csv\"), index=False)\n",
    "print(f\"[CALIB] ECE(Top-1) = {ece:.4f}\")\n",
    "\n",
    "# --- Threshold sweep e CSV final (mesmo τ que você usou) ---\n",
    "def sweep(df_, taus=np.arange(0.05,0.61,0.05)):\n",
    "    rows=[]\n",
    "    for t in taus:\n",
    "        m = df_[\"conf_top1_blend\"]>=t\n",
    "        cov = m.mean()\n",
    "        acc = (df_.loc[m,\"pred_top1_blend\"]==df_.loc[m,\"destination_departure\"]).mean() if cov>0 else np.nan\n",
    "        rows.append({\"tau\":float(t),\"coverage\":float(cov),\"acc_given_tau\":float(acc if not math.isnan(acc) else np.nan)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "thr_df = sweep(df)\n",
    "thr_df.to_csv(os.path.join(OUT_DIR,\"threshold_sweep_full_blend.csv\"), index=False)\n",
    "print(\"[OK] threshold_sweep_full_blend.csv salvo\")\n",
    "\n",
    "TAU = 0.35\n",
    "out = df.copy()\n",
    "out[\"accepted\"] = (out[\"conf_top1_blend\"] >= TAU).astype(int)\n",
    "out[\"in_vocab\"] = in_vocab_mask.astype(int)\n",
    "final_csv = f\"predicoes_full_blend_thresholded_no_prior_t{TAU:.2f}.csv\"\n",
    "out.to_csv(final_csv, index=False)\n",
    "print(f\"[OK] CSV final salvo: {final_csv}\")\n",
    "\n",
    "with open(os.path.join(OUT_DIR,\"metrics_full_blend.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"top1_overall\": float(acc_overall),\n",
    "        \"top3_overall\": float(top3_overall) if not math.isnan(top3_overall) else None,\n",
    "        \"top1_in_vocab\": float(acc_in) if not math.isnan(acc_in) else None,\n",
    "        \"top3_in_vocab\": float(top3_in) if not math.isnan(top3_in) else None,\n",
    "        \"ece_top1\": float(ece),\n",
    "        \"threshold\": float(TAU),\n",
    "        \"coverage_at_tau\": float((out['accepted']==1).mean())\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] Métricas salvas em {OUT_DIR}/metrics_full_blend.json\")\n"
   ],
   "id": "7a9588cdc159488b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPLIT] IN-VOCAB=1.000 | OUT-OF-VOCAB=0.000\n",
      "[RESULT] Top-1(overall)= 0.3253 | Top-3(overall)= 0.5633\n",
      "[RESULT IN-VOCAB] Top-1= 0.3253 | Top-3= 0.5633\n",
      "[CALIB] ECE(Top-1) = 0.0459\n",
      "[OK] threshold_sweep_full_blend.csv salvo\n",
      "[OK] CSV final salvo: predicoes_full_blend_thresholded_no_prior_t0.35.csv\n",
      "[OK] Métricas salvas em eval_report/metrics_full_blend.json\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T17:48:38.461681Z",
     "start_time": "2025-08-19T17:48:23.084419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==== eval_temporal_segmentos.py ====\n",
    "import os, ast, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "PRED_CSV = \"predicoes_full_blend.csv\"  # se quiser avaliar o thresholded, troque aqui\n",
    "ARTS    = \"artifacts_destino_model/artifacts.joblib\"\n",
    "OUTDIR  = \"eval_report\"\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- util ----------\n",
    "\n",
    "def _parse_list(s):\n",
    "    \"\"\"Tenta converter para lista de strings (suporta JSON ou lista em string com aspas simples).\"\"\"\n",
    "    if isinstance(s, (list, tuple, np.ndarray)):\n",
    "        return [str(x) for x in s]\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    st = str(s).strip()\n",
    "    try:\n",
    "        return [str(x) for x in json.loads(st)]\n",
    "    except Exception:\n",
    "        try:\n",
    "            return [str(x) for x in ast.literal_eval(st)]\n",
    "        except Exception:\n",
    "            # fallback: único rótulo como string\n",
    "            return [st]\n",
    "\n",
    "def top1_acc(y_true, y_pred):\n",
    "    return float((y_true == y_pred).mean())\n",
    "\n",
    "def topk_acc(y_true, y_topk_lists):\n",
    "    ok = []\n",
    "    for yt, lst in zip(y_true, y_topk_lists):\n",
    "        lst = _parse_list(lst)\n",
    "        ok.append(yt in lst)\n",
    "    return float(np.mean(ok))\n",
    "\n",
    "# ---------- carrega ----------\n",
    "\n",
    "arts = load(ARTS)\n",
    "le_dest = arts[\"le_dest\"]\n",
    "# demais encoders e modelo ficam disponíveis se precisar\n",
    "\n",
    "dfp = pd.read_csv(PRED_CSV)\n",
    "\n",
    "# Normaliza colunas esperadas\n",
    "need_cols = {\"purchase_datetime\",\"destination_departure\",\n",
    "             \"pred_top1_blend\",\"pred_topk_labels_blend\"}\n",
    "missing = need_cols - set(dfp.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltam colunas em {PRED_CSV}: {missing}\")\n",
    "\n",
    "# Datas\n",
    "dfp[\"purchase_datetime\"] = pd.to_datetime(dfp[\"purchase_datetime\"], errors=\"coerce\")\n",
    "dfp[\"ym\"] = dfp[\"purchase_datetime\"].dt.to_period(\"M\").astype(str)\n",
    "dfp[\"hour\"] = dfp[\"purchase_datetime\"].dt.hour\n",
    "\n",
    "# Garante comparabilidade dos rótulos: todos como strings (mesmo “vocabulário” do treino)\n",
    "# Caso o ground-truth esteja codificado numérico, decodifica com le_dest\n",
    "y_true_raw = dfp[\"destination_departure\"].astype(str)\n",
    "# Heurística: se é tudo dígito e dentro do range do encoder, decodifica:\n",
    "if y_true_raw.str.fullmatch(r\"\\d+\").all():\n",
    "    y_true_ids = y_true_raw.astype(int).to_numpy()\n",
    "    try:\n",
    "        y_true = pd.Series(le_dest.inverse_transform(y_true_ids))\n",
    "    except Exception:\n",
    "        y_true = y_true_raw.astype(str)\n",
    "else:\n",
    "    y_true = y_true_raw\n",
    "\n",
    "y_pred1 = dfp[\"pred_top1_blend\"].astype(str)\n",
    "y_topk  = dfp[\"pred_topk_labels_blend\"]\n",
    "\n",
    "# ---------- métricas globais ----------\n",
    "acc1 = top1_acc(y_true, y_pred1)\n",
    "acc3 = topk_acc(y_true, y_topk)\n",
    "print(f\"[GLOBAL] Top-1={acc1:.4f} | Top-3={acc3:.4f}\")\n",
    "\n",
    "# ---------- por mês ----------\n",
    "rows = []\n",
    "for ym, g in dfp.groupby(\"ym\"):\n",
    "    yt = y_true.loc[g.index]\n",
    "    yp = y_pred1.loc[g.index]\n",
    "    yk = y_topk.loc[g.index]\n",
    "    a1 = top1_acc(yt, yp)\n",
    "    a3 = topk_acc(yt, yk)\n",
    "    rows.append({\"ym\": ym, \"n\": len(g), \"top1\": a1, \"top3\": a3})\n",
    "\n",
    "temporal_df = pd.DataFrame(rows).sort_values(\"ym\")\n",
    "temporal_df.to_csv(os.path.join(OUTDIR, \"temporal_by_month.csv\"), index=False)\n",
    "print(f\"[OK] temporal_by_month.csv salvo ({len(temporal_df)} linhas)\")\n",
    "\n",
    "# ---------- por origem (top N por volume) ----------\n",
    "TOPN = 30\n",
    "by_origin = (dfp\n",
    "             .assign(_ok_top1=(y_true==y_pred1).astype(int))\n",
    "             .groupby(\"origin_departure\")\n",
    "             .agg(n=(\"origin_departure\",\"size\"),\n",
    "                  acc=(\"._ok_top1\", \"mean\"))\n",
    "             .rename(columns={\"._ok_top1\":\"acc\"})\n",
    "             .sort_values(\"n\", ascending=False)\n",
    "             .head(TOPN)\n",
    "             .reset_index())\n",
    "by_origin.to_csv(os.path.join(OUTDIR, \"by_origin_top30.csv\"), index=False)\n",
    "print(f\"[OK] by_origin_top30.csv salvo\")\n",
    "\n",
    "# ---------- por hora ----------\n",
    "by_hour = (dfp\n",
    "           .assign(_ok_top1=(y_true==y_pred1).astype(int))\n",
    "           .groupby(\"hour\")\n",
    "           .agg(n=(\"hour\",\"size\"),\n",
    "                acc=(\"._ok_top1\",\"mean\"))\n",
    "           .rename(columns={\"._ok_top1\":\"acc\"})\n",
    "           .reset_index()\n",
    "           .sort_values(\"hour\"))\n",
    "by_hour.to_csv(os.path.join(OUTDIR, \"by_hour.csv\"), index=False)\n",
    "print(f\"[OK] by_hour.csv salvo\")\n",
    "\n",
    "# Resumo de console\n",
    "print(\"\\n[MÊS] (últimos 6)\")\n",
    "print(temporal_df.tail(6))\n",
    "print(\"\\n[ORIGEM] Top 10\")\n",
    "print(by_origin.head(10))\n",
    "print(\"\\n[HORA]\")\n",
    "print(by_hour.head(10))\n"
   ],
   "id": "d6d34876059e94b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL] Top-1=0.3253 | Top-3=0.5633\n",
      "[OK] temporal_by_month.csv salvo (128 linhas)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['._ok_top1'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 101\u001B[39m\n\u001B[32m     96\u001B[39m \u001B[38;5;66;03m# ---------- por origem (top N por volume) ----------\u001B[39;00m\n\u001B[32m     97\u001B[39m TOPN = \u001B[32m30\u001B[39m\n\u001B[32m     98\u001B[39m by_origin = (\u001B[43mdfp\u001B[49m\n\u001B[32m     99\u001B[39m \u001B[43m             \u001B[49m\u001B[43m.\u001B[49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_ok_top1\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m==\u001B[49m\u001B[43my_pred1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    100\u001B[39m \u001B[43m             \u001B[49m\u001B[43m.\u001B[49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43morigin_departure\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m \u001B[43m             \u001B[49m\u001B[43m.\u001B[49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43morigin_departure\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m                  \u001B[49m\u001B[43macc\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m._ok_top1\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmean\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    103\u001B[39m              .rename(columns={\u001B[33m\"\u001B[39m\u001B[33m._ok_top1\u001B[39m\u001B[33m\"\u001B[39m:\u001B[33m\"\u001B[39m\u001B[33macc\u001B[39m\u001B[33m\"\u001B[39m})\n\u001B[32m    104\u001B[39m              .sort_values(\u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m, ascending=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    105\u001B[39m              .head(TOPN)\n\u001B[32m    106\u001B[39m              .reset_index())\n\u001B[32m    107\u001B[39m by_origin.to_csv(os.path.join(OUTDIR, \u001B[33m\"\u001B[39m\u001B[33mby_origin_top30.csv\u001B[39m\u001B[33m\"\u001B[39m), index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    108\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[OK] by_origin_top30.csv salvo\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1432\u001B[39m, in \u001B[36mDataFrameGroupBy.aggregate\u001B[39m\u001B[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001B[39m\n\u001B[32m   1429\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mengine_kwargs\u001B[39m\u001B[33m\"\u001B[39m] = engine_kwargs\n\u001B[32m   1431\u001B[39m op = GroupByApply(\u001B[38;5;28mself\u001B[39m, func, args=args, kwargs=kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m1432\u001B[39m result = \u001B[43mop\u001B[49m\u001B[43m.\u001B[49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1433\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_dict_like(func) \u001B[38;5;129;01mand\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1434\u001B[39m     \u001B[38;5;66;03m# GH #52849\u001B[39;00m\n\u001B[32m   1435\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.as_index \u001B[38;5;129;01mand\u001B[39;00m is_list_like(func):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:190\u001B[39m, in \u001B[36mApply.agg\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_str()\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_dict_like(func):\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43magg_dict_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(func):\n\u001B[32m    192\u001B[39m     \u001B[38;5;66;03m# we require a list, but not a 'str'\u001B[39;00m\n\u001B[32m    193\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agg_list_like()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:423\u001B[39m, in \u001B[36mApply.agg_dict_like\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    415\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34magg_dict_like\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> DataFrame | Series:\n\u001B[32m    416\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    417\u001B[39m \u001B[33;03m    Compute aggregation in the case of a dict-like argument.\u001B[39;00m\n\u001B[32m    418\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    421\u001B[39m \u001B[33;03m    Result of aggregation.\u001B[39;00m\n\u001B[32m    422\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m423\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43magg_or_apply_dict_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43magg\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1603\u001B[39m, in \u001B[36mGroupByApply.agg_or_apply_dict_like\u001B[39m\u001B[34m(self, op_name)\u001B[39m\n\u001B[32m   1598\u001B[39m     kwargs.update({\u001B[33m\"\u001B[39m\u001B[33mengine\u001B[39m\u001B[33m\"\u001B[39m: engine, \u001B[33m\"\u001B[39m\u001B[33mengine_kwargs\u001B[39m\u001B[33m\"\u001B[39m: engine_kwargs})\n\u001B[32m   1600\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m com.temp_setattr(\n\u001B[32m   1601\u001B[39m     obj, \u001B[33m\"\u001B[39m\u001B[33mas_index\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m, condition=\u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[33m\"\u001B[39m\u001B[33mas_index\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1602\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1603\u001B[39m     result_index, result_data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_dict_like\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1604\u001B[39m \u001B[43m        \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1605\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1606\u001B[39m result = \u001B[38;5;28mself\u001B[39m.wrap_results_dict_like(selected_obj, result_index, result_data)\n\u001B[32m   1607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:462\u001B[39m, in \u001B[36mApply.compute_dict_like\u001B[39m\u001B[34m(self, op_name, selected_obj, selection, kwargs)\u001B[39m\n\u001B[32m    460\u001B[39m is_groupby = \u001B[38;5;28misinstance\u001B[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001B[32m    461\u001B[39m func = cast(AggFuncTypeDict, \u001B[38;5;28mself\u001B[39m.func)\n\u001B[32m--> \u001B[39m\u001B[32m462\u001B[39m func = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnormalize_dictlike_arg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    464\u001B[39m is_non_unique_col = (\n\u001B[32m    465\u001B[39m     selected_obj.ndim == \u001B[32m2\u001B[39m\n\u001B[32m    466\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m selected_obj.columns.nunique() < \u001B[38;5;28mlen\u001B[39m(selected_obj.columns)\n\u001B[32m    467\u001B[39m )\n\u001B[32m    469\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m selected_obj.ndim == \u001B[32m1\u001B[39m:\n\u001B[32m    470\u001B[39m     \u001B[38;5;66;03m# key only used for output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:663\u001B[39m, in \u001B[36mApply.normalize_dictlike_arg\u001B[39m\u001B[34m(self, how, obj, func)\u001B[39m\n\u001B[32m    661\u001B[39m     cols = Index(\u001B[38;5;28mlist\u001B[39m(func.keys())).difference(obj.columns, sort=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cols) > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m663\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mColumn(s) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(cols)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m do not exist\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    665\u001B[39m aggregator_types = (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mdict\u001B[39m)\n\u001B[32m    667\u001B[39m \u001B[38;5;66;03m# if we have a dict of any non-scalars\u001B[39;00m\n\u001B[32m    668\u001B[39m \u001B[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001B[39;00m\n\u001B[32m    669\u001B[39m \u001B[38;5;66;03m# be list-likes\u001B[39;00m\n\u001B[32m    670\u001B[39m \u001B[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001B[39;00m\n",
      "\u001B[31mKeyError\u001B[39m: \"Column(s) ['._ok_top1'] do not exist\""
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
